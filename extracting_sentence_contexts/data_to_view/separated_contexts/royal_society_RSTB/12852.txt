In the auto-associator case, time to learn is inversely proportional to the square of the strength of the dimension.

simplified network are qualitatively similar to the dynamics sigmoid curve starting from an initial value a;(0) to its
of learning in the highly nonlinear deep network we first asymptotic value s;.
explored in McClelland ef al. [7] and reviewed above. Figure 6 shows the values computed from the theory for
Indeed, the analysis we describe here was developed in these equations as a function of f for the choice s;(0)=0.001
Saxe et al. [26] to provide a theoretical understanding of pat- and for a value of z dependent on the network learning rate,
terns that had previously only been observed in simulations. as described in electronic supplementary material, Â§SI. We rep-
The truly remarkable fact about the dynamics of learning in resent t in normalized time units corresponding to the number
a deep linear network is that it is completely characterized by of epochs, or complete sweeps through the training set, times
the SVD, subject to an influence of the initial values of the con- the learning rate, which we choose very small to produce a
nection weights at the beginning of the learning process. That faithful approximation to the continuous learning equations.
is, if the training of the network proceeds as in our earlier simu- This allows us to capture the continuous nature of the learning
lations with the original Rumelhart network, such that each and to highlight that the choice of the actual learning rate
item is presented once in each training epoch, the network's simply determines the time scale of the process. These curves
input-output (IO) matrix at a particular time f measured in are superimposed on a plot of the actual dynamics of learning
epochs will be characterized by the two equations below and ina simulation of a network like the one shown in figure 5.