predictions closely matches the validation data (2 Hz pacing), giving a (false) confidence in the model performance. (c) Notably,
Model F gives catastrophic predictions for the /x, block (CoU) experiments (suggesting the validation data are not an appropriate
test given the intended model use). The posterior predictions are model predictions made using parameter values sampled from
the posterior distribution (figure 3); here, 200 samples/predictions are shown, but they overlay and are not distinguishable by
eye. (Online version in colour.)

posterior distribution, estimated by MCMC, and the point estimates obtained by optimization
are shown in figure 3. The posterior distribution is very narrow (note the scale), which suggests
that we can be confident about the parameter values. The resulting posterior predictions, shown
in figure 2c, give a very narrow bound. By ignoring model discrepancy we have become highly
(and wrongly) certain that the catastrophically bad predictions are correct.

It is worth noting that all of the issues above arise from the fact that the model discrepancy
was ignored during calibration. In the scenario of no model discrepancy, i.e. when fitting Model T
to the data, none of the issues above occurred, as shown in electronic supplementary material,
figure S1.

To conclude our motivation of this paper, we can see that neglecting discrepancy in the model’s
equations is dangerous and can lead to false confidence in predictions for a new context of use.
We discuss methods that have been suggested to remedy this in §3.

(c) Astatistical explanation

To understand what happens when we fit an incorrect model to data, let us first consider the
well-specified situation where the data generating process (DGP) has probability density function