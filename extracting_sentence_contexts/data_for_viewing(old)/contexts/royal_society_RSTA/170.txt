One contribution of 11 to a Theme Issue ‘Statistical challenges of high-dimensional data’.

4237 This journal is © 2009 The Royal Society


4238 I. M. Johnstone and D. M. Titterington

imited to the ‘small p, large n’ scenario. This scenario also naturally reflected
he contemporary limitations of computers (the term meant people prior to 1950)
and graphical display.

A natural mode for asymptotic approximation therefore imagines that n— co
while p remains of smaller order than n, in fact usually fixed. Among the most
amiliar theoretical results of this type are the Laws of Large Numbers and the
Central Limit Theorems. The former says that the sample mean of a random
sample of size n from a population has as a limit, in a well-defined sense, the
population mean, as n tends to oo. The corresponding central limit theorem
shows that the limiting distribution of the sample mean about the population
mean (when scaled up by ./7) is of the normal or Gaussian type. In statistics, such
results are useful in deriving asymptotic properties of estimators of parameters,
out their validity relies on there being, in theory at least, many ‘observations
per parameter’.

In practice, n will generally correspond to the number of experimental units
on which data are available; for p, however, there are at least two, albeit related,