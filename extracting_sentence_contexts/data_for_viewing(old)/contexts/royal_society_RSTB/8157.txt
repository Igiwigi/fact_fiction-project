test cases correct even when half of the training labels
were wrong. Like a good student, the net ignores the
supervisor when the supervisor is obviously wrong but

where 0 < 1; (t) < 1 is a real-valued approximation to
the stochastic binary state v; and 0<A<1 must be
large enough to prevent oscillations. After computing
the reconstruction, the hidden states are again
sampled using 1; and r; in place of v; and v; in equation
(7.2). The contrastive divergence learning rule for the
third-order weights is then

still makes good use of the fact that the supervisor is _ ;

better than random. This is possible because the unsu- Acvin © (Pi2j)tk)aata — (Tithe reconstruction” (7.4)

pervised learning reveals natural classes in the data and This way of allowing hidden units to modulate

the role of the supervisor is primarily to name these interactions between visible units has far too many

natural classes rather than to define them. parameters because each hidden unit has independent
control over every pairwise interaction between the vis-

7. A BETTER MODULE FOR DEEP LEARNING ible units. For real images, however, we expect the
