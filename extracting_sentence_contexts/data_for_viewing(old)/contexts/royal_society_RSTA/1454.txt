experiments. Together they underpin the scientific method and strongly interact. Theory provides
the necessary framework for computational models, experiments provide the data against which
the computational models need to be validated, and numerical simulations may lead to new
insights and theory or new hypotheses that are tested experimentally.

In performing simulation-based science, we usually go through a generic modelling and
simulation cycle (figure 1). Based on available (observational) data and knowledge and theory
of the phenomenon under study, a conceptual model is formulated, which is then turned into a
computational model. This is then implemented on a computer after which we perform numerical
experiments with the computational model. These simulations provide results that are used to
validate our models and, once validated, to predict the behaviour of the system we study.

The power of this approach lies in the fact that the computational sciences have developed
a large collection of well-established generic modelling paradigms (such as ODE or PDE-based
methods, particle-based methods, agent-based methods, fully discrete methods, etc.) and a large
collection of well-established (numerical) methods to discretize these computational models and
implement them very efficiently on the full range of available computers (from the laptop via
cloud to petaflop/s supercomputers). We argue that also in multiscale modelling and simulation
such generic approaches are possible and have been demonstrated in a number of successful
projects. Having been exposed to such solutions over the last decade, we will discuss the potential
of generic multiscale modelling and simulation, with emphasis on high-end performance levels,
projecting forward to the era of exascale computing.


spatial scale spatial scale ' / Ao '