A different framework of dependent components in time series was proposed by Lahat
et al. [58], combining the idea of independent subspaces discussed earlier with suitable
non-stationarities.


(d) Further models of dependencies

A model in which the components are linearly correlated (without considering any time structure)
was proposed by Sasaki et al. [59]. The idea is to consider a generative model similar to the one
in (6.2), but with, in some sense, opposite assumptions on the underlying variables: the s; are
linearly correlated, while the v; can be independent (above, it was approximately vice versa).
This changes the statistical characteristics because 5; are zero-mean while the v; are non-negative.
In fact, the s; are then linearly correlated. A topographic kind of dependencies was proposed by
Sasaki et al. [59].

Very general kinds of dependencies can be modelled by non-parametric models. However,
such as all non-parametric models, estimation may require very large amounts of data.
A framework modelling dependencies in the form of trees and clusters was proposed by Bach &

Jordan [60]. A related approach was proposed by Zoran & Weiss [61]. 13

A recent trend in machine learning is ‘deep learning’, which means learning multi-layer : =
models, where each ‘layer’ is a linear transformation followed by a nonlinear function taken : ca
separately of each linear component, like in a neural network [62-64]. In fact, many such models : go
can be considered to be related to ICA: ICA essentially estimates one layer of such a ae