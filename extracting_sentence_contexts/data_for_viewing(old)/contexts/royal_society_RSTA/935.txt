
In May 2018 the new General Data Protection Regulation (GDPR) became law throughout
the European Union. This regulation has a number of facets, and details can be found
at the ICO https: //ico.org.uk/media/for-organisations / documents /2013559 /big-data-ai-ml-
and-data-protection.pdf. There is a new concept of a lawful basis for processing, as well as
guidelines on automated individual decision-making and profiling. Additional explanation of


the application of machine learning is directly available from the ICO https: / /ico.org.uk/media/
for-organisations / documents /2013559 /big-data-ai-ml-and-data-protection.pdf.

When decisions are made automatically, the data processor must ensure (that if requested)
human intervention is available, as well as an explanation. GDPR recital 71 in fact states: ‘The
data subject should have the right not to be subject to a decision, which may include a measure,
evaluating personal aspects relating to him or her which is based solely on automated processing
and which produces legal effects concerning him or her or similarly significantly affects him or
her.’ This seems like an overall exemption from automated processing, but the regulation also
specifies which scenarios are in turn exempt from this specification (for example, when subjects
given their assent). Much has been written about the ‘right to an explanation’, language which by
some authors is considered to be somewhat imprecise [74].

GDPR is also discussed by Blacklaws [2] in this discussion issue. In particular, the :
predictability and clarity of opaque complex algorithms can work against easily generated [=
explanations. An open question is if the predictive framework can produce an explanation—an :3
area which is the subject of considerable research [49,75], as is the interpretation of the right to an :