nonlinear transformations being uncorrelated, i.e.

cov(fi (s1),f2(s2)) = Effi (s1)f (s2)} â€” Effi (si) }E{f(s2)} = 0, (6.1)

for any nonlinear functions f, and fy, with E{.} denoting mathematical expectation. This is
in stark contrast to uncorrelatedness, which means that (6.1) holds for the identity function
fils) =f2(s) =s. This equation suggests that to find a transformation that is guaranteed to give
independent components, we need an infinite number of parameters, i.e. a class of rather
arbitrary nonlinear transformations. It is thus not surprising that linear transforms cannot achieve
independence in the general case, i.e. for data with an arbitrary distribution. (See Hyvarinen et al.
[38, ch. 9] for more discussion.)

In fact, if we consider a real dataset, it seems quite idealistic to assume that it could be a linear
superposition of strictly independent components. A more realistic attitude is to assume that
the components are bound to have some dependencies. Then, the central question is whether
independence is a useful assumption for a particular dataset in the sense that it allows for
estimation of meaningful components. In fact, empirical results tend to show that ICA estimation
seems to be rather robust against some violations of the independence assumption.

On the other hand, modelling dependencies of the estimated components is an important
extension of the analysis provided by ICA. It can give useful information on the interactions
between the components or sources recovered by ICA. Thus, the fact that the components are
dependent can be a great opportunity for gaining further insights into the structure of the data.

