Sporns [11], effective information is mathematically well-described [8,9,12]. While it keeps
being re-invented as a measure of causation [13,14], generally without acknowledgement of
previous formulations or ongoing lines of research, the measure has not yet been proven


to be the unique measure of causation, and there are alternative proposals (many of which
are mathematically related) for how to measure causation using information theory [15].
Indeed, the same general phenomena of causal emergence have been shown in integrated
information [16,17] as the Â¢ measure can increase at macroscales due to similar reasons of
increasing determinism and decreasing degeneracy of state transitions. Measures that are
not directly causal but capture aspects of causation, like assessing the entropy of random
walkers on networks (indicating uncertainty of transitions), can also improve at macroscales
in that random walkers are more deterministic in their dynamics [18]. The fact that causal
emergence occurs across multiple measures indicates there is a broader phenomenon at work.
Specifically, there is somehow more causally relevant information at macroscales (although it
is currently unclear if this is captured by a unique measure of causation, or is better captured
by a set of common measurements). Here, we explore broadly how such information gain
is possible, and demonstrate it in the well-understood measure of the mutual information
itself.

Of course, information cannot be created ex nihilo. Therefore, we propose that emergence is a
form of information conversion at a higher scales. When measured in its totality in a given system,
total information measures like the entropy of the distribution system states, the Kolmogorov
information for describing the entirety of the system, or the total correlation in the form of the
mutual information, all necessarily decrease at a macroscale (or at best, do not decrease, but can