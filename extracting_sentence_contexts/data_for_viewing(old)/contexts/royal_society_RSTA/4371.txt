do not know. That would be the ‘unknown unknowns’ that a certain political figure once injected
into the national discourse.

These probability distributions become non-uniform (that is, some states are more likely
than others) once you acquire information about the states. This information is manifested by
conditional probabilities. You really only know that a state is more or less likely than the random
expectation if you at the same time know something else (like in the case discussed, whether the
driver is texting or not). Put another way, what I am trying to tell you here is that any probability
distribution that is not uniform (same probability for all states) is necessarily conditional. When
someone hands you such a probability distribution, you may not know what it is conditional
about. But I assure you that it is conditional. I will state it as a theorem:

All probability distributions that are not uniform are in fact conditional probability distributions.

This is not what your standard textbook will tell you, but it is the only interpretation of ‘what
do we know’ that makes sense to me. ‘Everything is conditional’ thus, as the title of this section
promised.

We can also write down what the average uncertainty for crashing your car is, given your
texting status. It is simply the average of the uncertainty while texting and the uncertainty while
not texting, weighted by the probability that you engage in any of the two behaviours. Thus,


the conditional entropy H(X | Y), that is the uncertainty of crashing your car given your texting | 10 |
status, is