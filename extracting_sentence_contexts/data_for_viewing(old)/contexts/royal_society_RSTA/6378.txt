specifically to ‘refactor’ their representations during training to decrease description length
of more general concepts [64]. [65] go even further in arguing that in-context learning—the
primary mechanism via which LLMs exhibit sample efficiency—can be understood as an implicit
implementation of more familiar Bayesian inference. Across all these studies, the fact that LLMs
might obey similar principles during their training to the principles obeyed by probabilistic
symbolic systems provides evidence that the processes they use under the hood may reflect those
that traditional theories in cognitive science routinely employ.

As ever, skepticism about the sample efficiency of LLMs is warranted. Models of this size are
often proprietary, and even when the data is publicly available, determining whether a task or
concept is ‘unseen’ is not trivial [66]. These claims demand further study. Still, based on what
we know right now, pretrained LLMs are not obviously incompatible with human-like sample
efficiency, and in fact may be governed by similar constraints as competitive symbolic models in
similar settings.


(e) Summary and discussion

A common criticism of neural networks as candidate models of the human mind is that neural
networks lack the abstract symbolic structures and processing algorithms necessary to explain
human language. Such claims are typically supported by evidence of neural network performance
failures on tasks that traditionally require symbolic processing. I argue that if we focus instead on
research which seeks to assess the underlying competence of neural network models—i.e. work
which characterizes the structure of the representations that models use under the hood—the
picture is more positive, with neural networks reflecting many characteristic aspects of symbolic