This is what Zarsky [26] 1m
terms ‘interpretability’; disclosing an algorithm, for example, will probably only convey meaning a
to a technologist working in the field, so any explanation will need to translate what the aes
AI is doing in terms which are meaningful to the recipient of the explanation. Such an .
explanation might even make no attempt to explain the algorithm at any level of abstraction,
if doing so would not achieve the purpose for which the explanation is required. Wachter
et al. [27] point out that what they describe as ‘counterfactuals’, examples of changes to the
facts which would have produced a different decision by the AI, can be more illuminating
to some recipients if the aim is to enable those recipients to understand why a decision
was made. Any legal requirement to incorporate transparency into AI needs to take account of these
differences in perspective. Merely demanding transparency is meaningless without the context
of the human who requires transparency.