
Of course, failsafe scenarios also exist, whereby different sets of weights work even though
they differ considerably from each other, because all local minima are basically equivalent quasi-
solutions of the optimization problem. We suspect, without proof, that such a form of benevolence
lies at the basis of the most remarkable successes of machine-learning and modern artificial
intelligence applications.

But this cannot be assumed to be the universal rule: a big red light is there in general, the name
of the game being ‘overfitting’, that is a stiff solution exists which reproduces very well a given set


Figure 3. Examples of three classes of landscapes that may be encountered in connecting input to output variables, shown
in three dimensions but typically, in fact, arising in much higher dimensions which cannot be drawn. (a) A relatively smooth
(i.e. continuous) landscape over which machine-learning algorithms might be expected to perform well; (b) a fractal landscape
which is not differentiable and contains structure on all length scales; (c) another discontinuous landscape with no gradients.
Neither (b,c) would be expected to perform reliably in the context of machine-learning algorithms. Reproduced from Coveney
etal. [7].

of data, but fails grossly as soon as the dataset is enlarged, if only slightly. Although well known
to the machine-learning community, these problems are typically swept under the carpet by the
most ardent BD aficionados.

5. The two distant sisters: correlation and causation

The fact that correlation does not imply causation is such a well-known topic that we only