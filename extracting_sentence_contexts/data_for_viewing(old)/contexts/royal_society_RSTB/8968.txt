bold-italic typeface to indicate true states of the world and that prescribe action. Here, the generative model plays a dual
italic typeface for hidden or fictive states assumed by an role—it is a predictive model over observations and encodes
agent. The parameters (expectations) of categorical distri- optimal policies (in terms of prior beliefs about control
butions over discrete states s € {1,..., J} are denoted by J x 1 states). The agent and the environment interact in cycles. In
vectors §  [0, 1], while the ~ notation denotes sequences of each cycle, the agent first figures out which hidden states are
variables over time. most likely by optimizing its expectations with respect to the
free energy of observations. After optimizing its posterior
Definition. Active inference rests on the tuple (Q, S, A, P, beliefs, an action is sampled from the posterior marginal over
P,Q,R, S, U) control states. The environment then picks up this action,
generates a new observation and a new cycle starts.
— A finite set of observations . The optimization above is that usually portrayed in terms of
— A finite set of true states and actions S x A. perception (inference about hidden states) and action (a choice
— A finite set of fictive or hidden states S x U. model in which action is a function of inferred states). Action
— A generative process over observations, states and action and perception couple the agent to the environment; where
RG, §, a) = Pr ({00,..., 01} = 6, {80,-.-, $1} = §, expectations depend upon observations—through perception,
{ao,.--, ar} = 4). whereas observations depend upon expectations—through
— A generative model over observations and hidden states action. Usually, expectations are associated with neuronal
POG, §, ti|m) = Pr ({o9,..-, 01} = 0, {50,.--, 8:1} = 8, activity or connection strengths and action is associated with
{ur,..., Ur} = i). the state of effectors. In brief, expectations about the state
— An approximate posterior probability over hidden states with of the world minimize free energy, while action is selected
expectations p. © R? such that QG, t|)=Pr ({s0,..., 5} =8, from the ensuing posterior beliefs about control states.
{up,..., Ur} = Ui). The expression for free energy above shows that it upper

bounds the negative logarithm of Bayesian model evidence
— In P(6|m) or surprise. This is because the relative entropy or