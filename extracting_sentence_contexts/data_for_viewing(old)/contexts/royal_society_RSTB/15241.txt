missed and of which is asked to predict the missing part (output). When predictions are accurate, the model is ‘rewarded’ (its
accuracy scored through an assigned number) and then fine-tuned with an increasing quantity of texts, missing larger portions.


Finally, performances are evaluated, validated and tested on datasets to ensure the model generalizes accurately to unseen data.
Through different degrees of supervision, the network is optimized for effectively predicting the most likely output, based
on associations and contrasts between inputs and data in its training set. That is, the model can predict, with a high level of
accuracy, the portion of the missed text, which corresponds to answering the question asked by a user. LLMs are powerful
tools: users leverage the (approximately) 2.25 billion pages contained in the indexed web and, through ‘prompt engineering’,
craft their prompts in targeted ways to exact useful responses. The capabilities of LLMs have already been applied to a wide
range of cases, such as the ability to decode the meaning of perceived and imagined speech and silent video [65]. Results are
impressive, but they also highlight inherent limitations, particularly the flexibility in transferring knowledge from one set of
data to another, which also bring about states of ‘artificial hallucinations’ (i.e. generated responses that are either factually
incorrect, non-sensical or not grounded in reality (e.g. [66-68]).

Current LLM systems do not implement forms of active learning and autonomous engagement with the world’s affordances.
Even if the system could be able to recover facts about the world’s structure, the recovered facts are explicitly based on the
4E human experience, hence the depiction of LLMs as ‘stochastic parrots’, due to their pattern-matching approach to the
language detailed above [69].* Others consider LLMs only an outward display of intelligence: similarly to Searle’s ‘Chinese
room’, they give the impression of comprehending only because they can match incoming input symbols with specific output
symbols [74]. Obviously, it is not possible to say that disembodied entities such as LLMs could never achieve understanding or
agency in some sense. The production of systems able to describe the reasons for their decisions (i.e. explainability) in natural
language, as a pragmatic premise to overt discourse, is a major avenue for Als’ development [14—16]. However, even if LLMs
could be considered bona fide cognitive agents, this (as we noted earlier) does very little to undermine the position of 4E and
AIF accounts of cognition. On the one hand, the ‘ground’ of LLMs’ understanding will be explicitly based on the 4E human