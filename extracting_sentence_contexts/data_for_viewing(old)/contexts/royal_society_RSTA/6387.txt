and coordinate their thoughts and actions with human thoughts and actions’. It may well be
the case that showing that LLMs exhibit intelligence is a higher bar than showing they encode
meaning, and it may be that communicative intent and other types of grounding will play a
crucial role in such debates. However, LLMs’ failure to account for all of human cognition (e.g.
full-blown intelligence) does not prevent them from serving as useful models of some aspects
(meaning). And, in fact, seriously investigating the potential of LLMs to shed light on the former
may incidentally generate insights about the latter.

4. Conclusion

It is an open question whether the success of LLMs can offer insight on the study of language
understanding in humans. Two common arguments against LLMs as models of humans are (i)
the fact that LLMs lack the capacity to represent abstract symbolic structure and (ii) the fact that
LLMs are trained only on text and thus lack grounding. Support for both claims is typically based
on either in-principle arguments, or else on evidence of LLMs performing poorly on tasks that
require symbols or grounding, respectively.

In this article, I argued that neither criticism of LLMs can be accepted a priori, and rather, both
claims must be tested empirically. In particular, for those interested in the potential of LLMs to
model cognition, the priority must be on characterizing the models’ underlying competence, rather
than focusing on measures of their performance (good or bad). Recent empirical work on the former
gives reason to believe that neural networks can learn to encode many key aspects of traditional
symbolic structures, and that even ungrounded language models can encode a conceptual space
that is structurally similar to a grounded space. Overall, I conclude it is premature to make
claims about intrinsic (in)abilities of LLM. Rather, the next decade of empirical work is likely