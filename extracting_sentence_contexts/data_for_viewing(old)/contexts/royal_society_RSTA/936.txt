potential to eliminate unfairness in this setting. Examples of unfairness is one where data density
varies with wealth, such as the aforementioned example of identifying the need for street repairs
[54]. The public debate includes examples with disparate impacts in low- and middleincome
countries [90,91], and clearly our understanding of global data flows and their impacts is still
in its infancy.

Finally, fairness is intricately related to the notion of reproducible research in science. If
scientific results are not reproducible, the question of fairness becomes moot, as everyone is in
some sense potentially treated arbitrarily. Discussion has been ongoing in the social sciences in
this area [38-40]. In part, this is a consequence of the statistical analysis tools for observational
data not having caught up with the large volumes of such data that are now routinely collected,
as well as publication biases. Schuemie et al. [9] explore, using a version of meta-analysis, finding
the appropriate way of combining multiple observational studies, and coping with the fact that
the empirical distribution of p-values is nothing like the analytic null models we are used to
applying. Much is needed in the way of new mathematical theories and statistical methodologies
to handle scenarios such as these, and it seems that many lessons can be gleaned from considering
the challenges faced by observational studies in human health and in the social sciences.

5. Accountability and agency in algorithmic decision-making

Accountability of algorithmic decision-making is a popular topic of investigation. A dictionary
definition of ‘accountability’ will tell you that the word corresponds to the state of being liable or
answerable. The importance of accountability is highlighted in several recent reports [43,73,92].
The Public Policy Council of the Association for Computing Machinery [43] notes the importance
that institutions that use algorithmic decision-making are responsible for the decisions they make.