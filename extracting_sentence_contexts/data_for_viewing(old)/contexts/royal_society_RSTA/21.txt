the right-hand side of (2.1), H = an w;5e, represents the mixing measure. Yet another equivalent
representation, useful for clustering, makes use of allocation variables z:

. ind iid
Vilzi =f iW ~ FCG W), zi Catwr,..., 27),

where Cat(-) represents the categorical distribution with parameter w. In the Bayesian setting, the
model is completed with a prior on the unknown parameters w, 6 and y (or equivalently on the
unknown mixing measure H and ).

In order to obtain clusters of practical relevance, the kernel f(-|@, 7) should be carefully selected
to reflect the shape and properties of a cluster for the application at hand. A standard choice is
the multivariate Gaussian distribution, f(-|0, ~) = N(- | 4j, 4j). In fact, the widely used k-means
algorithm can be seen as a limiting case of the EM algorithm for Gaussian mixture models, where
the kernel is N(- | Mj, 071) [40,41]. This highlights that k-means imposes restrictive cluster shapes,
specifically, all clusters have the same spherical shape of equal size in all dimensions, with only
the centres j4; allowed to differ across clusters. More generally, Gaussian mixture models relax
this assumption by allowing different ellipsoidal shapes and sizes across clusters. The cluster-
specific covariance matrices can be parametrized as T=) jDjAjDI, where dj, Dj andA i control the
volume, orientation and shape, respectively, of the ellipsoid and each parameter can be cluster-
specific or global for general geometric cross-cluster constraints [14,42]. Other types of constraints
on the covariance matrices can also be considered, such as mixtures of factor analysers [43,44] and
mixtures of Gaussian graphical models within a casual framework [45,46].

