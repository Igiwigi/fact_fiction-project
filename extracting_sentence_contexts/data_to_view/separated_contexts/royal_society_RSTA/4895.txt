The first is the belief that if the subject matter is susceptible to uncertainties (‘greater or smaller
errors’ [1, p. 311]) then precise definition of ideas or concepts is either impossible or unnecessary. Of course, with the benefit of the hindsight accumulated from observation of the immense
advances that statistical tools have led to, one might nowadays state that if the subject matter
is susceptible to uncertainties then the demand for statistical methods is all the greater. Although
Rutherford had an element of truth in his famous observation that ‘if your experiment needs
statistics, you ought to have done a better experiment’, he missed the fact that any investigation
at the boundaries of knowledge must, almost by definition, have uncertainties and errors in
measurement—and hence needs statistics to tease out the answers. Fisher secondly attributed the dire state of early twentieth century statistics to the confusion
arising from using the same word for the unknown true value of a parameter and its estimate. Indeed, although the word ‘parameter’ was occasionally used in physics at the time, it was
virtually unknown in statistics.