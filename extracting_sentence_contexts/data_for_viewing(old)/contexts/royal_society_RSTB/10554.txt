regularization, e.g. setting a to 0.1 or 0.001—in general, we expect
our results to hold as long as learners have some bias for regularity,
but that bias is not so strong that it completely overwhelms their data.
See Burkett & Griffiths [23] for further discussion and results for
Bayesian iterated learning in populations.

Because in the models and experiments we present here, all nouns
occur equally frequently, we can simply calculate entropy by noun
and then average (yielding the term 1/O in the expression
above)—if nouns differed in their frequency, then the by-noun entro-
pies would be weighted proportional to their frequency, rather than a
simple average. Similarly, in the expression for H(Marker|Noun,
Speaker), we exploit the fact that all speakers are represented equally
frequently in the models and experiments presented here.

“In all conditions, to convert test output from the generation g partici-
pant into training input for generation g + 1, for a given scene, we
simply inspected whether the generation g participant used fip, tay or
no marker, and used this marking when training participant n + 1.
Insituations where the marker was mistyped, we treated it as if the par-
ticipant had produced the closest marker to the typed string, based on
string edit distance (e.g. ‘tip’ treated as ‘fip’). Errors in the verb or noun
used were not passed on to the next participant.

SA logit regression on the data from the two-person condition, with