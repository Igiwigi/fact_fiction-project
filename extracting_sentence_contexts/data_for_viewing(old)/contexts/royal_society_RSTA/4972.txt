optimum’. He says, ‘My treatment of this problem differs radically from that of Bayes. Bayes
[11] attempted to find ... the actual probability that the population value lay in any given range.

... Such a problem is indeterminate without knowing the statistical mechanism under which
different values of p come into existence ... What we can find from a sample is the likelihood of



any particular value of p, if we define the likelihood as a quantity proportional to the probability
that, from a population having that particular value of p, a sample having the observed value
r, should be obtained’ [10, p. 24]. And again he stresses the difference in Fisher [1, p. 326]:
‘likelihood is not here used loosely as a synonym of probability, but simply to express the
relative frequencies with which such values of the hypothetical quantity p would in fact yield
the observed sample’.

He goes on to characterize the distinction between the two concepts: ‘Formally, therefore,
[likelihood] resembles the calculation of the mode of an inverse frequency distribution. This
resemblance is quite superficial: if the scale of measurement of the hypothetical quantity be
altered, the mode must change its position, and can be brought to have any value, by an
appropriate change of scale; but the optimum, as the position of maximum likelihood may be
called, is entirely unchanged by any such transformation’ [1, p. 327]. This is an elaboration of
the point he made about scale transformation in Fisher [9]. He stresses that likelihood ‘is not a : =
differential element, and is incapable of being integrated: it is assigned to a particular point of the 3
range of variation, not to a particular element of it’ [1, p. 327].
