
5However, this openness only breeds understanding insofar as there are enough people to examine the available code.

$While it may seem as though computers, being entirely deterministic machines, would give rise easily to reproducible
systems, this is often not the case for many of the same reasons that give rise to practical inscrutability [3].


4. Explanation, understanding, validation and policy interventions

Another approach to defeating inscrutability which many scholars have proposed is the use of
explainable systems, which provide an explanation of their outputs [4,5,30,49,50]. While these
technologies promise much in the way of making systems more trustworthy, good explanations
require careful consideration of who receives the explanation and what facts those people must
extract from the explanations. Explanations need not relate mechanistically to the operation of the
system they are explaining, though they often do. Any system is a model of the world, and too
often the called-for explanation is only a description of that model. In some cases, a mechanistic
description of how a system computed a particular output will sufficiently explain that output. In
other cases, as with human decision-makers, it is sufficient to produce an unrelated justification of
the result or a general description of how it was reached. Explanations can be intended either to

improve the collaboration between humans and machines, or to help subjects and non-subjects of : =
a computer system believe in the validity of the outputs of that system. In either case, explanations 3
must create understanding by humans of computer systems in context, though they are not on 8
their own necessary or sufficient to create that understanding. : =
