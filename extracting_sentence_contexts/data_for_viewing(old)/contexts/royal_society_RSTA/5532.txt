is an increasing interest in the use of assurance cases to justify the safety of ML-based systems,
particularly for automotive [76] and healthcare applications [77]. The notion of explainability,
particularly pre-deployment, could form a key part of an ML assurance case used to explain
and justify, e.g. to regulators, key decisions about the choice of the ML model and quality and
suitability of the data sets. Post-deployment, local XAI methods could help to implement a highly
dynamic assurance case [78,79] where the critical predictions made by an ML-based system could
be used to update the assumptions about, and confidence in, the system deployed compared with
the assessment made pre-deployment.

8. Conclusion

ML-based systems are already being used in situations that can have an effect on human well-
being, life and liberty. This, combined with the fact that they move decision-making away
from humans, makes it an assurance imperative to provide evidence that this transference is
appropriate, responsible and safe. Explanations of ML-models and ML-generated predictions can
be provided as part of this evidence. But they sit within a wider accountability framework, in
which human decision-makers are still required to give the normative reasons or justifications
(which XAI methods cannot provide) for the ML-models. Our analysis of stakeholder needs
and the contrast with the capabilities of XAI methods gives, we believe, a starting point for
understanding how to employ explainability in an assurance role. Assurance of ML-based
systems deployed in living environments has an ethical dimension. This is reflected in the
underlying ethical nature of the reasons—to inform consent, to challenge an unfair prediction,
to assess confidence before implementing a decision that, if wrong, could harm the recipient—
for which stakeholders might require visibility of an ML-model or an explanation of one of its
predictions. We hope that this paper will help shift the balance of work on XAI methods from