Humans correctly inter- in light of the syntax [56], without, importantly, sharing the = =

pret the word ‘it’ in this sentence as referring to the trombone implication of the phrase ‘good enough’ that the syntactically = 5!

if the last word is ‘large’ but to the suitcase if the last word is determined interpretation is necessarily the best one. In gen- >

‘small’. We believe that a semantic regularity—the fact that eral, it seems beneficial to rely on all possible sources of =

for an object (a) to fit into another object (b), (a) must be smaller information rather than giving overriding importance toa = &

than (b)—underlies this ability, and this is just the kind of regu- single consideration. In line with this view, rates of plausibility =

larity we believe a fully successful model based on the ideas based interpretations are higher when listening to speakers S

underlying the SG model should capture, while Winograd sen- with a foreign accent, suggesting that such interpretations are N

tences pose challenges for even the best current deep learning not a failure of the system, but rather result from a Bayes opti- s

models, for which training is purely language based [24]. mal process taking into account all available cues to best =
There are some important differences in the behaviour of the estimate the intended meaning, integrating noisy evidence

prediction error concerning the next word (word surprisal [49- and semantic priors [57].

51]) as measured by most current neural network models of These considerations play into an important contrast

language [45] and the change in a probabilistic representation between the SG model and an alternative model by Brouwer

of meaning in the SG model. Specifically, word surprisal is sen- et al. [39], which is grounded in the classical view that syntax

sitive to both semantic and syntactic regularities and violations, necessarily has a decisive role in interpretation.