
In this case, we have to proceed in an explicitly Bayesian fashion, because (2.12) is effectively a
highly structured prior on X, and the posterior distribution will be a conjunction of the constraints
in the likelihood and the prior,

r(x | 2°PS) oc w(x, 295) = L(x) (x | x0) 1 (X0)- (3.5) : a)

This is a generalization of the Q = 0 case in the sense that as Q > 0, so x becomes a deterministic : a
function of xg, and (x | x9) becomes a Dirac 5-function. : 2
It is easy to see that the problem becomes harder when we allow for structural uncertainty, 8

in situations where there is sensitive dependence on initial conditions and an attractor (now a : &
random attractor). This is implicit in the fact that this case is a generalization of the previous case, 1S
but with many more uncertain quantities. 3
For ease of comparison, continue to focus on learning Xo, using the integrated likelihood : =

K(xo) =| Le m(x |X) dx. (3.6a)

In the simplest treatment, this integral can be approximated with a finite sample of size m,

m .
K(xo) © K(xo) = m7! > Lae?), x ig m(x| Xo). (3.6b)
j=l

For each xo, each candidate trajectory x” will typically approach the attractor, which also contains