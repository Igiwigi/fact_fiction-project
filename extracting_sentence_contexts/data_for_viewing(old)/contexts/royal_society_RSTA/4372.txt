
Figure 2. Entropy Venn diagram shows conditional and mutual entropies of two variables. Source: Wikimedia. (Online version
in colour.)

Ah, yes. That is a good diagram. Two variables X and Y. The red circle represents the entropy
of X, the blue circle the entropy of Y. The purple thing in the middle is the shared entropy I(X : Y),
which is what X knows about Y. Also what Y knows about X. They are the same thing.

You wrote I(X : Y) but Wiki says I(X; Y). Is your semicolon key broken?

Actually, there are two notations for the shared entropy (a.k.a. information) in the literature. :
One uses the colon, the other the semicolon. Thanks for bringing this up. It confuses people. In :s
fact, | wanted to bring up this other thing ...

Hold on again. You also keep on saying ‘shared entropy’ when Wiki says ‘shared information’. You really
ought to pay more attention.

Well, you. That is a bit of a pet-peeve of mine. Just look at the diagram above. The thing in the
middle, the purple area, it is a shared entropy. Information is shared entropy. ‘Shared information’
would be, like, shared shared entropy. I think that’s a bit ridiculous, don’t you think?

Well, if you put it like this. I see your point. But why do I read ‘shared information’ everywhere?

That is, dear reader, because people are confused about what to call entropy, and what to call
information. A sizable fraction of the literature calls what we have been calling ‘entropy’ (or