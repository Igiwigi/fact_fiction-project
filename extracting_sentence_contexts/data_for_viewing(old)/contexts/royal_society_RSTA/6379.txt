
GPT-3 has no idea how the world works. ..The immense database of things that GPT draws
on consists entirely of language uttered by humans, in the real world with utterances that
(generally) grounded in the real world. That means, for example, that the entities. ..and
properties. . generally refer to real entities and properties in the world . . .[but] GPT doesn’t
actually know which of the elements appropriately combine with which other properties
[70].

Such arguments are extremely compelling because they are based on a premise that is impossible
to disagree with. When humans learn and use language, they are embedded in a rich non-
linguistic world, interacting via perception, communication, planning and goals. Text-only
language models are not. However, the face-validity of the premise hides the subtlety of what
is at-issue in the conclusion. In fact, there is not consensus among philosophers and cognitive
scientists about the extent to which grounding is a key component of what we conventionally
refer to when we use the word ‘meaning’.

Here, it is worth differentiating claims about the need for communicative intent from claims [2
about the need for an external world more generally. (Perception is the most obvious example : >
of the latter category, but there are other examples, e.g. an agent’s non-communicative goals.) :8
Arguments against LLMs often entangle both into a generic notion of grounding, but these claims : -
are not the same. They inherit from separate philosophical traditions and thus have separate sets : <
of criticisms to address. (cf. [71,72] who present partially overlapping arguments with the ones I _
make below.) : 8

SI