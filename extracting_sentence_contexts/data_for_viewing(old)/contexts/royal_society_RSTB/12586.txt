
A given dependency coding network can retrieve and constituents in the superposition grows. Thus, the degree of S
recode items from arbitrary positions in the linear sequence mismatch, and any activation requisite for such an encoding,
buffer, provided it has sufficient integrative power. Therefore, is likely to decrease over time.
this representational scheme is capable of encoding not just We propose that hierarchical structure-building is one of
nested dependencies, but other types of hierarchical depen- the key roles of the dorsal aspect of ventrolateral prefrontal
dency such as crossed dependencies. For example, figure 3 cortex (dorsal VLPFC, incorporating Brodmann areas
(right diagram) illustrates retrieval of items 1 and 4 (Al and 44 and 45). This position is supported by human neurobiologi-

B1) from sequence memory, which are chunked through bind- cal evidence on syntactic processes prominently featuring
ing and superposition; and retrieval of items 2 and 3 (A2 and hierarchical dependencies [34,38]. It has been proposed that
B2), which are likewise chunked. These two chunks form a at least BA44 supports a recursive, multi-dependency manage-
superchunk representing a nested dependency. Cross-serial ment process, a fact that would explain increases in its
dependencies between words are notably also present in activation observed with increasing depths of hierarchical
some human languages. Like nested dependencies, these can dependency nesting [114]. The recursive reuse of a consistent
also be readily represented by our model. We could, for architecture by VLPFC would be consistent with our recursive
example, have retrieved and bound items 1 and 3, and 2 and use of vector symbolic operations during the encoding of
4, respectively, to form a crossed dependency structure. hierarchical dependencies. Repeated superposition of sparse
This is illustrated within our coded demonstration (doi:10. dependency representations will manifest as an increase in
5281 /zenodo.3464607). The inclusion of cross-serial depen- local activity as increasing numbers of neurons support the
dencies alters the minimum computational requirements for representation; as an example of this effect, consider the
any parsing agent [110], and thus it is important that a increased activity represented at the top of figure 3, relative
domain-general representational model has the potential to to the bottom. A ‘reset’ (or re-sparsification) of the buffer will
account for them as well as other language-like hierarchical result in a sudden drop in population activity. Such neural
constructions. accumulation and reset activity has been identified within