them. These questions are paramount to our understanding of meaning in language and
cognition, and research in this direction could substantially influence the as yet philosophical
debates on the issue.

Of course, there are many questions beyond only whether LLMs can serve as models of
meaning. As [83] argues, ‘intelligence is inherently social, and artificial systems, like humans,
will be viewed as intelligent, and will be valued as collaborators, to the extent that they can align
and coordinate their thoughts and actions with human thoughts and actions’. It may well be
the case that showing that LLMs exhibit intelligence is a higher bar than showing they encode
meaning, and it may be that communicative intent and other types of grounding will play a
crucial role in such debates. However, LLMs’ failure to account for all of human cognition (e.g.
full-blown intelligence) does not prevent them from serving as useful models of some aspects
(meaning). And, in fact, seriously investigating the potential of LLMs to shed light on the former
may incidentally generate insights about the latter.

4. Conclusion

It is an open question whether the success of LLMs can offer insight on the study of language
understanding in humans. Two common arguments against LLMs as models of humans are (i)
the fact that LLMs lack the capacity to represent abstract symbolic structure and (ii) the fact that
LLMs are trained only on text and thus lack grounding. Support for both claims is typically based
on either in-principle arguments, or else on evidence of LLMs performing poorly on tasks that
require symbols or grounding, respectively.

In this article, I argued that neither criticism of LLMs can be accepted a priori, and rather, both