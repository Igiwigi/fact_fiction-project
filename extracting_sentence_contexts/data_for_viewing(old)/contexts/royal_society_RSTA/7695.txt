2


STATISTICAL ESTIMATION 345

Va, = 6 (F; — 6))*,

does not exceed that of any other unbiassed estimate of 0,.

It is known that Marxorr provided a remarkable theorem leading, in certain
cases, to the calculation of the best of the unbiassed estimates which are linear
functions of the variables (4). The advantage of the unbiassed estimates and the
justification of their use lies in the fact that in cases frequently met the probability of
their differing very much from the estimated parameters is small.

x The other principle, which is to a certain extent in rivalry with that of the
Sunbiassed estimate, is the principle of maximum likelihood. This consists in con-
Bidering L = const. X p (x';, *’g... x’,[0,... 9,), where x’; denotes the observed
value of X;, as a function of the parameters 0;, called the likelihood. It is advocated
‘that the values of L may serve as a measure of our uncertainty or confidence in the
“torresponding values of the Os. Accordingly, we should have the greatest con-
“idence in the values, say, 9,, 9, ... 9,, forwhich Lis a maximum. 9; obviously is
<A function of x’,...x',; it is called the maximum likelihood estimate of 6;.
5 As far as I am aware, the idea of the maximum likelihood estimates is due to
‘Kart Pearson, who applied the principle in 1895 (see particularly pp. 262-265),