difficult or impossible to examine due to fundamental mathematical limitations within computer : =
science [15,16]. But even here, systems can be designed to sidestep these limits and to support that : a
interrogation—inscrutability remains a choice that can be accommodated for in system design. 73
Rather than discounting systems which cause bad outcomes as fundamentally inscrutable and aes
therefore uncontrollable, we should simply label the application of inadequate technology what 22
it is: malpractice, committed by a system’s controller. [4

Another common approach to dealing with inscrutability is to eschew attempts to understand : *
the system at all and to treat outcomes from computer systems as though they were unforeseeable : S
externalities analogous to pollution [17,18] or other sources of nuisance [19]. This analogy leads : &

scholars to look for remedies such as efforts-based liability regimes and to describe the impact
of biases in decisions made by machines as unavoidable, structural, environmental fact. But
arguing that the effects of a system are unforeseeable pre-apologizes for that system's failures.
While demanding perfect foresight puts too much faith in engineering, it is certainly true
that systems which are designed to achieve certain goals in a trustworthy and verifiable way
can reasonably be judged on an absolute scale. In many domains where perfect foresight is
impossible and where the outcomes from interventions are not foreseeable, such as medicine,
we nonetheless hold practitioners to a standard of practice and are comfortable punishing
practitioners when their actions do not rise to a sufficient standard of care. Further, the law
does successfully regulate harmful environmental effects of processes in other domains, requiring
that systems which might have negative consequences be controlled by the best available
technology [20].

2. Intensional understanding: goals, requirements and mechanisms