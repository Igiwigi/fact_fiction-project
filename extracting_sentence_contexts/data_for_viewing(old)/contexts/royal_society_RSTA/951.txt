unavailable, unsatisfying or ineffective. On this basis, we may envisage the malfunctioning of AI
robots or their manipulation that cause or fuel human wrongdoing, if not properly detected and
recovered, therefore making people vulnerable to systematic recourse to such artificial agents.

Still, there are three problems with this kind of narrative on robots ‘just like corporations’. First,
the corporate solution for the legal agenthood of AI robots is only one among several technical
options that scholars have suggested over the past years. The list of models proposed by the
‘Artificial Intelligence and Robotics Experts’ in their letter from April 2018 is incomplete. There is
no such a simple alternative between the ‘Legal Entity model’ and the ‘Anglo-Saxon Trust model’.
Scholars have discussed over the past years about registries for artificial agents [2], insurance
policies [20] and modern forms of the ancient Roman legal mechanism of peculium, namely the
sum of money or property granted by the head of the household to a slave or son-in-power [5,
p- 104]. As a matter of fact, what all these examples suggest is that legal systems can properly
address the challenges of the agenthood of AI robots in contracts and business law, by making


them accountable, without resorting to any form of corporation and hence, any kind of legal
personhood of AI robots. We return to this below in §§4d and 6a.

Second, the extent of the legal personhood of corporations varies among legal systems.
Contrary to the US tradition, for example, most EU companies do not enjoy their own privacy
rights, or their own political rights, such as freedom of speech [8]; corporations cannot be held
criminally responsible in the civil, as opposed to the common, law tradition [11], etc. All in all,
legal systems can hold the big companies of Silicon Valley accountable for what their AI systems
do, and yet some of these AI systems could still be conceived of as, say, ‘data processors’ pursuant