from this assumption. Nonetheless, it is a start. Arguably it also suggests a way to select a value

for k: since the ridge estimate (2.17) is optimal under the model (2.14)-(2.19) it seems natural 3
to use the same modelling assumptions to select k, for example using an EB approach (i.e. by oS
maximizing the marginal likelihood). [8

(iii) Lasso regression

The lasso regression estimate [30] is obtained by replacing the squared /? penalty of ridge 15
regression in (2.18) with an fp penalty, |B|1 = va Bj : S
Bs? = arg min —I() + KiB ht. 220) :8
It is common to describe B!***° as the Bayesian ‘maximum a posteriori’ (MAP) estimate for 8 under aed
a Laplace prior (also known as double-exponential prior), a fact that is straightforward to verify. a]
This description is, in essence, an examination of lasso through a Bayesian lens. =
However, some cautionary remarks are in order. First, and most importantly, the MAP estimate :s
is ultimately not a very ‘Bayesian’ quantity, at least not for continuous parameters. Why is this? to
Well, the MAP estimate is the optimal estimate under 0-1 loss, which is not a natural loss 4)
function for continuous parameters. By contrast, the posterior mean estimate is optimal under a
squared loss, which is much more natural. Second, lasso is often applied in settings where it is 1 8
believed that many of the regression coefficients are equal to (or very close to) 0; the Laplace prior a)
does not really seem to capture this belief. Thus one might, perhaps uncharitably, characterize :8
the lasso estimate as corresponding to the Bayes estimate under an inappropriate prior and an :2
inappropriate loss function.
These remarks might suggest asking a different question: ‘Under what prior is the lasso
estimate the posterior mean’? I believe the answer to this is that no such prior exists. Because