will be less than 35h. Secondly, future reuse of the work will be significantly easier which has
the effect of lowering barriers to collaboration and building on previous work. Gains from these
factors can make new student onboarding and collaboration with other groups more efficient, as
well as imparting greater confidence that the results are computationally correct.

(c) Implementation of Cl tests

As discussed previously, for each of the seven articles, we constructed tests to be run on Travis CI,
a service for continuous integration (CI) available at https: //travis-ci.org. We constructed these
tests based on our attempts at reproducing the results from the articles. In general, we found it
very easy to implement tests which involved simply building and running the code successfully
on Travis. We implemented tests that compared the results of the computational experiments
against previously computed known answers, which took us 2-4h per article. In fact, most of the
time used for these articles consisted of writing an appropriate tool to parse output from these
computational tests and then compare it to the known results, the problem sometimes called
“test oracles’ [113]. The varied nature of the output of each individual test made it impossible
to write a general-purpose solution, so each test required a customized solution. With these
Cl tests, a researcher looking to use these code bases will be able to execute the tests on their
own machines and know that they have the correct code performing the correct calculations to a
specified level of accuracy. Where and how that accuracy level may be precisely set is a question for
future work.

Another issue that can arise with implementing CI tests is that researchers may write some
flaky tests, which are tests that can non-deterministically pass or fail when run on the same version
of code [114]. In general, flaky tests can mislead developers during software development: a flaky