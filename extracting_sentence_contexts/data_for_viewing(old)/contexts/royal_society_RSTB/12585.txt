' 1 DOG SUBJECT 3 DOG '
i; 2 EAT VERB 1 EAT : not trained
encoding _ ' '
1 3 STEAK] [PATIENT 2 STEAK :
So EAT 0fo[1] [of fo
test 4 SUBJECT DOG STEAK 0[0[0] [00/0
A DOG 0[0[0] [0[0]o
test role

Figure 3. Description of a sample trial in simulation 1. During encoding, the model was presented with a series of three role-filler pairs (SUBJECT - DOG, VERB - EAT,
PATIENT - STEAK). At the last (fourth) time step, the model was only presented with a test role (in the example above: SUBJECT) and it had to output the
corresponding filler which was associated with it (DOG). When the model was trained to produce VARS representations, the role-filler pairs were accompanied
by a random? permutation of tokens during encoding which determined the allocation of symbols to representational slots (for example, the fact that DOG
was paired to token 3 meant that DOG has to be represented in the third representational slot). In this way, the random token input ensured that each filler
has been allocated to each slot during training. The VERB was treated as a binary relation and its arguments (i.e. the relational roles SUBJECT and PATIENT)
were bound to the corresponding fillers (in this example, subject to DOG and PATIENT to STEAK). Note, the VARS output was only trained in time step 4,
which means that no error was computed during the encoding stage (the ‘not trained’ area).

Table 1. Combinatorial generalization means accuracy rates in simulations 1 and 2. (In both simulations, the models achieved combinatorial generalization only
when trained to explicitly represent symbolic structures. The standard deviation is shown in parentheses. CNN, convolutional neural network.)

combinatorial generalization accuracy (s.d.)

main task VARS task
simulation 1 LSTM 0.30 (0.15) na.