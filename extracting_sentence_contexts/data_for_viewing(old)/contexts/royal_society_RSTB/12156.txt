not sufficient for cognition, since non-living systems exhibit regenerative repair fit the predictions of surprise minimiz-
such properties as well (the example of the ball rolling ation models. Being a generic model of cognition, active
down a plane described above). We next describe specific inference approaches have also motivated a variety of aneural
applications of LA in characterizing the mechanisms of contexts, including self-organization in a primordial soup
certain cognitive systems. [19], and morphogenesis in a biological model of the flat-
Learning and inference in neural networks can be viewed worm [112]. Moreover, hierarchical forms of active inference
as applications of the LA principle [119-122]. If the edge have been proposed as models of specific forms of cognition
weights of an ANN represent an imaginary particle’s coordi- such as associative learning, homeostasis and allostasis =
nates, then the loss function (defined as the error between the [125,126]. >
observed output and the target) would represent the poten- Language—arguably the epitome of cognition—has a __
tial energy of the system of particles and the derivatives of unique statistical signature captured by ‘Zipf’s Law’, which = =:
the error with respect to the weights would represent the kin- is a power-law distribution, which expresses the empirical — 5'
etic energies of the particles [120,121]. Thus, learning in the fact that the frequency of a word is inversely proportional = =
neural network involves transforming the potential energy to its rank (the position of the word in a list ordered by fre- =
(error) into kinetic energies (weight alteration), together con- quency) [127]. For example, the most frequent word occurs =
stituting the ‘cognitive energy’. The principle of LA simply about twice as frequently as the second most frequent iat
helps compute the ‘cognitive action’ (the learning trajectory), word. Clearly, a language has many other features, and yet 3
from the constraint that the least amount of cognitive energy Zipf’s Law is almost universal, since many languages have
is spent. Such a process is also at play in the case of a neuron this property [127]. Of the various possible mechanisms 33
becoming a target for a neurotrophic factor, or when a new that could generate a power-law [128], those based on the &
synapse is generated [122]. Moreover, this view extends to principle of LA constitute some. In one study, a combination
aneural processes as well, like when bacteria display chemo- of least effort principles and information theory was used to
taxis or when humans acquire more resources over time, both formulate a generative mechanism. In particular, an assump-
targeted at consuming energy in the least time [122]. tion that a language-generating mechanism minimized
The ‘free energy principle’ which states that biological communication inefficiency and cost of producing symbols