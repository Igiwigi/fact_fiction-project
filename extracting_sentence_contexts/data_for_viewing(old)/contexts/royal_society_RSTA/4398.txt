history of preferences/ratings is limited. The classical techniques for making such inferences are
based on hierarchal Bayesian models and require high computational costs which render them
infeasible for large-scale applications. This process can be speeded up by leveraging tools from
machine learning, such as collaborative filtering techniques [53,54]. Alternatively, the average
demand can be learned online based on the instantaneous demand on a base station, in which case
the caching problem can be modelled as a multi-armed bandit problem [55]. Clearly, additional
methods are needed to further reduce the computational time of demand forecasting.


(b) Communication overhead |

There is also an overhead of cooperation among SBSs and coordination between SBSs and macro-
cells that needs to be taken into account when implementing caching mechanisms. In fact, if 3
multiple base stations collaborate to cache some file segments, this requires extra metadata ae
management and may increase user experienced delay. This is more crucial in highly mobile
networks where users are rapidly handed off between the base stations. This issue is outside
the scope of our study, and we leave it as a future work.

(c) Conflicting objectives

Finally, we emphasize that operators and users have conflicting objectives; operators aim at
reducing their servicing costs, whereas user satisfaction degrades with increasing experienced : =
delay. In the presented multicast service network, we capture the minimum acceptable level 3
of user satisfaction by the time period of the multicast service within which all pending user 12
requests are collected and served by the multicast stream. In the mobility-aware caching case, nee)