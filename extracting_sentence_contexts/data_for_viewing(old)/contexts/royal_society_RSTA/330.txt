or less implicit in the theory of classical factor analysis, where the components or factors are
assumed uncorrelated and Gaussian, which implies that they are independent (more on this
below). A physical interpretation of independence is also sometimes possible: if the components
are created by physically separate and non-interacting entities, then they can be considered
statistically independent.

On the other hand, the third assumption is not necessary and can be relaxed in different ways,
but most of the theory makes this rather strict assumption for simplicity.

So, the real fundamental departure from conventional multi-variate statistics is to assume that
the components are non-Gaussian. Non-Gaussianity also gives a new meaning to independence:
for variables with a joint Gaussian distribution, uncorrelatedness and independence are :
in fact equivalent. Only in the non-Gaussian case is independence something more than :z
uncorrelatedness. Uncorrelatedness is assumed in other methods such as PCA and factor analysis, [3
but this non-Gaussian form of independence is usually not. a

As a trivial example, consider two-dimensional data that are concentrated on four points: :8
(—1,0), (1,0), (0, —1), (0,1) with equal probability t The variables x; and x2 are uncorrelated [=
owing to symmetry with respect to the axes: if you flip the sign of x1, the distribution stays 1
the same, and thus we must have E{x,x2} = E{(—x,)x2}, which implies their correlation (and :g
covariance) must be zero. On the other hand, the variables clearly are not independent because if :§
x1 takes the value —1, we know that x2 must be zero. :

(c) Objective functions and algorithms
