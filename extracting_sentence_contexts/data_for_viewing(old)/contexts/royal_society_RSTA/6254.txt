This shows that an unbiased gradient of this ELBO w.rt. 6 can be obtained by building
a normalized importance sampling approximation pg(z |x) = vk, If 5x of pe(z|x) and
integrating Vp» log pe(x,z) w.rt. it. The exact same gradient estimator would be obtained by
replacing pg(z | x) in place of pg(z | x) in Fisher’s identity (1.2). This shows that the biased gradient
approximation of the true log-likelihood ¢(6, x) obtained by importance sampling is nothing but
an unbiased gradient of Liw(0,¢). The ELBO interpretation is more fruitful as it shows the exact
objective one optimizes when using stochastic gradient ascent. Additionally, it also allows us

1q(u) can depend on 6,¢ and x but this is notationally omitted.


to optimize the algorithmic parameter ¢. However, standard gradient estimates of this ELBO
for ¢ have poor properties as K increases due to the fact that Liw(0,¢) becomes then almost
independent of ¢ (because of the convergence of the ELBO to the true log marginal likelihood),
and thus decreasingly strongly the signal-to-noise ratio of the estimator of Vg Liw(@,¢) [21]. One
can remedy this behaviour by using a ‘sticking the landing’ variance reduction approach [22] or
by doubly reparameterizing the gradient estimates of V¢Liw(0, @) [23].

In this section, we have presented a generic way to obtain an ELBO from any unbiased
positive estimate of the evidence. However, we have so far limited ourselves practically to a basic
importance sampling estimate of the evidence which performs poorly in high dimension. In the
next sections, we discuss more sophisticated Monte Carlo techniques.

3. Annealed and sequential importance sampling
