identified above. First, control systems operate in time. Control
policies are functions, not discrete actions, and specify how the
controller co-evolves with the system. They are thus fundamen-
tally dynamic quantities. Second, control policies do not require
the semantics of decisions at all. They are conceptualized as
automatic processes that operate adaptively, often deterministi-
cally, once a goal has been set. Thus, they have intuitive appeal
for a framework that combines sparse, conscious decisions
with automatic ongoing processes.

Again, this approach is complementary to the RL per-
spective reviewed above [28], which likewise attempts to
learn a policy that maximizes some set of rewards. In fact,
many classic RL problems like cart-pole or learning to walk
are control problems [56,60,61]. Such links have often been
overlooked in decision neuroscience, where RL actions are
most frequently associated with decisions [62], but there is
nothing in the formalism to force this view. Before the
advent of deep RL, RL methods were often limited to low-
dimensional, discrete state and action spaces, but modern
applications now routinely feature complex, continuous pol-
icies parameterized by deep neural networks [30,63]. Thus,
RL likewise offers up the notion of a full policy in place of
a discrete action as a building block of continuous decisions.
