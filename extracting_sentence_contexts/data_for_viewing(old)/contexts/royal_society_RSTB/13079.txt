output pattern. On the left, the sum squared error over all eight patterns is shown as a function of normalized training time. On the right, the singular value
dimension strengths calculated from the network's output are shown using solid lines; the mathematically expected curves are shown as dashed lines. Discrepancies
between observed and expected vary from run to run of the simulation and are due to variation in the projections of the random initial weights in the network onto
the dimensions of the SVD of the item-property matrix. In the one-hot case, time to learn a dimension is inversely proportional to the strength of the dimension in
the training set. In the auto-associator case, time to learn is inversely proportional to the square of the strength of the dimension.

simplified network are qualitatively similar to the dynamics sigmoid curve starting from an initial value a;(0) to its
of learning in the highly nonlinear deep network we first asymptotic value s;.
explored in McClelland ef al. [7] and reviewed above. Figure 6 shows the values computed from the theory for
Indeed, the analysis we describe here was developed in these equations as a function of f for the choice s;(0)=0.001
Saxe et al. [26] to provide a theoretical understanding of pat- and for a value of z dependent on the network learning rate,
terns that had previously only been observed in simulations. as described in electronic supplementary material, Â§SI. We rep-
The truly remarkable fact about the dynamics of learning in resent t in normalized time units corresponding to the number
a deep linear network is that it is completely characterized by of epochs, or complete sweeps through the training set, times
the SVD, subject to an influence of the initial values of the con- the learning rate, which we choose very small to produce a
nection weights at the beginning of the learning process. That faithful approximation to the continuous learning equations.
is, if the training of the network proceeds as in our earlier simu- This allows us to capture the continuous nature of the learning
lations with the original Rumelhart network, such that each and to highlight that the choice of the actual learning rate
item is presented once in each training epoch, the network's simply determines the time scale of the process. These curves
input-output (IO) matrix at a particular time f measured in are superimposed on a plot of the actual dynamics of learning
epochs will be characterized by the two equations below and ina simulation of a network like the one shown in figure 5. For
the corresponding curves presented in the top panel of figure 6. the simulation, we employ a network with 32 hidden units,
iar initializing the weights with small random values such
TOW) = Siai(u'v", (21) that their SVD can be characterized by 32 random initial
where dimensions with an average initial strength of 0.001. The