A wide variety of approximation methods have been developed to solve Bayesian inference
problems. These can be roughly divided into stochastic approximations (which make extensive
use of random numbers) and deterministic approximations. Some examples of widely used
stochastic approximate inference methods include MCMC methods (for an excellent review, see
Neal [72]), exact sampling methods [73] and particle filtering methods [74]. Some examples
of deterministic algorithms include the Laplace approximation, variational methods [75] and
expectation propagation [76]. Both deterministic and stochastic algorithms for inference can often
exploit the conditional independence relationships that exist between the variables in a model to
perform the relevant computations efficiently using local messages passed between nodes of a
graphical model [77-79].

'8Most theoretical results on intractability focus on the worst case of a problem instance. The real-world inference problem
may in fact be much easier to approximate.


A complete review of approximate inference methods is beyond the scope of this paper, but
a couple of points are worth making. All approximate inference methods can be characterized
in terms of a speed-accuracy trade-off. Some methods are fast but often inaccurate, while
other methods are slow or, such as MCMC, can be run for increasing amounts of time to give
increasingly accurate results. There is no general rule of thumb for which approximate inference
method is bestâ€”different models and problems tend to favour different methods. However, for a
particular problem, the difference between a good choice of inference algorithm and a poor choice
can be orders of magnitude of computation. Thus, it is well worth being familiar with a number
of inference algorithms and being willing to try several methods on a particular problem.
