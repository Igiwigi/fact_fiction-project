

compressions of richer perceptual representations. They are not regions of state space activated by each label are adjusted to | 8 |

neutral labels, but pointers to richer contents. Combinatorial fit with one another.

operations are defined on the pointers that are reversible, allow- According to this suggestion, the syntactic structure into

ing the components to be recovered (the pointer is which working memory labels are combined furnishes

‘dereferenced’). This is like the tensor product architecture semantic constraints on the types of scenario that will be

suggested by Smolensky [63]. A vector for dog is convolved simulated. In fact, much of the work in linguistics on compo-

with a vector for agent so as to form a representation of dog- sitional semantics is focused on pinning down these

as-agent. The identity of the agent (dog) can be extracted from constraints (in the case of language comprehension). Syntax

the convolution and decompressed. Blouw et al. [64] use circular constrains the type of scenario with which a thought can be

convolution for vector combination, which can be applied fleshed out. Jerry Fodor's classic view is that there is a

recursively (although the components, since they are com- language module that parses linguistic input, outputting a

pressed, are only imperfectly recoverable). Halford et al. [43] linguistic form, which is just a structured representation of © =