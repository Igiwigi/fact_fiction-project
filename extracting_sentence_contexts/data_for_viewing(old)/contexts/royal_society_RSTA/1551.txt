overarching direction and the Reproduction Package can be widely implemented successfully.

Since we interacted with authors and attempted to implement missing features or methods
ourselves, our work describes only what is possible given 40h of human time and solid
motivation to reproduce the results. It’s possible that different results would be achieved if the
researcher reproducing the results was more skilled in the given field than us, or didn’t have time
to implement anything themselves, or if the authors were more or less responsive to requests for
comment. We endeavoured to provide a clear exposition of how much effort is needed to gain at
least some amount of reproducibility under specific circumstances.

As mentioned in the Methods section, our time measurement was subject to human error. Our
experience indicates that more results are unlikely to be reproduced in just a few additional hours
of work after 40. Further compounding uncertainty here is the fact that over time we gained skill
in reproducing these articles. It is possible that an hour of time on the last article we reproduced
was more productive than an hour of time spent on the first article.

6. Conclusion

In our experience, the articles and approaches studied in this work showcase widespread
issues with scientific code sharing throughout the scientific community. As discussed in
the Incentives section, the routine production of fully reproducible published results is not
common practice. From our empirical investigations, we contribute three broad principles for
reproducible computational research: provide transparency regarding how computational results
are produced; when writing and releasing research code, aim for ease of (re-)executability; and
make any code upon which the results rely as deterministic as possible. We present 12 guidelines