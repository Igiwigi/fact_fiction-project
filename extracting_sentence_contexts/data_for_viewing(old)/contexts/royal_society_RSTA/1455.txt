
As the clock speeds of individual cores are no longer increasing, emerging exascale machines
can only reach their anticipated exaflop/s computational speeds by aggregating larger and larger
numbers of nodes and cores. As a result, these machines are becoming ‘fatter’, not faster. We
have argued before that as a result of this we are approaching the limits of what is achievable
using monolithic codes [31]. Our argument was that ‘because the parallelism is usually applied
to the spatial domain, we are increasingly simulating larger slabs of matter and bigger chunks
of the Universe, applying weak scaling by using more particles, a higher grid resolution or more
finite elements. Yet it often is the temporal behaviour that one is really interested in, and that
behaviour is not extended by adopting larger computers of this nature, or by making the problem
physically larger. Since the scientific problems of interest usually have time scales which scale
as a nonlinear function of the volume of the system under investigation, each temporal update
requires more wall clock time for larger physical problems. This is in fact a recipe for disaster: we
are not getting closer to studying large space and long time behaviour with monolithic codes’. We
consider this to be a cogent argument to suggest that monolithic codes could be the exception on
exascale machines, as weak scaling may not produce the desired results. If so, we should invest
in developing other scenarios, including high-performance multiscale computing, or scenarios
where monolithic codes (or coupled multiscale codes for that matter) are repeated over and over
again in ensembles, e.g. in parameter sweeping scenarios or for uncertainty quantification (see
also Portegies Zwart, who recently made a similar argument [27]).

We will now further analyse this argument by looking in detail at different scaling scenarios,
relying on the Scale Separation Map as introduction in figure 2 and high-level modelling of
parallel performance.
