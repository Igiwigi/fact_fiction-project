help it decide on risk levels by way of textual analysis, of hospital admission reports for instance.
If it intervenes in respect of a child at low risk, and if this assessment was unreasonable due to
issues with the tool, then it will be acting outside its powers (not to mention reducing services
to children at real risk of harm and causing unnecessary disruption and stress to the child and
family).

In this context, the work of Ribeiro, Singh and Guestrin [42] is of interest. Their experiment—
in respect of classifiers that were trying to determine if a document was about Christianity or
atheism—used a dataset that, despite a high accuracy on validation data, contained features
that did not generalize, and thus validation accuracy overestimated real-world performance [42].
When test set accuracy was used as a measure of trust in order to choose between two text
classification models, users tended to select the worse classifier, i.e. the classifier that, despite
achieving a high percentage accuracy rate, in fact had serious issues ‘in the wild’ due to the
importance given to irrelevant words (such as ‘Posting’ and ‘Re’). When individual prediction
explanations were shown, however, it was possible for a human user with prior knowledge to see

‘Short v. Poole Cpn. [1926] Ch. 66.


if a prediction was made for arbitrary reasons unconnected with the purpose of the prediction,
and so take steps to improve an untrustworthy classifier [42].

(c) Risk assessments and predictions

Riberio et al.'s experiment was in respect of a definitional classification. Risk assessments