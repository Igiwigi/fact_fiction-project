It may well be
the case that showing that LLMs exhibit intelligence is a higher bar than showing they encode
meaning, and it may be that communicative intent and other types of grounding will play a
crucial role in such debates. However, LLMsâ€™ failure to account for all of human cognition (e.g.
full-blown intelligence) does not prevent them from serving as useful models of some aspects
(meaning). And, in fact, seriously investigating the potential of LLMs to shed light on the former
may incidentally generate insights about the latter.

4. Conclusion

It is an open question whether the success of LLMs can offer insight on the study of language
understanding in humans. Two common arguments against LLMs as models of humans are (i)
the fact that LLMs lack the capacity to represent abstract symbolic structure and (ii) the fact that
LLMs are trained only on text and thus lack grounding.