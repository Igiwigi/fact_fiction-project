By replacing the batched dense DA
matrix-vector routines by corresponding batched dense matrix matrix (GEMM) routines, the
hierarchical matrix multiplication operation inherits all the performance benefits of the latter. This effect is more pronounced on GPUs because of their deeper memory hierarchies, and the
high parallelism and coalesced memory accesses of the compute-bound GEMMs. In fact, the
multiple vector operation can achieve a substantial portion (greater than 90%) of the batched
GEMM performance, which on the P100 is about 1.6 TFLOP/s in double and 2.8 TFLOP/s in
single precision. Next, we consider a low-rank update operation, since a number of algorithms in the library
use an operation of the form A= A+ XY as a building block. The low-rank update could be
a global update where X and Y are dense matrices of size n x r, or could be a set of local
low-rank updates affecting only a portion of the matrix A, where rows of X and Y represent
a subset of the n matrix rows.