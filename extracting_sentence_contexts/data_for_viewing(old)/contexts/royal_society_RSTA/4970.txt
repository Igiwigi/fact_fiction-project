
After giving a list of definitions, Fisher’s 1922 paper begins by commenting on ‘the prolonged
neglect into which the study of statistics, in its theoretical aspects, has fallen’. It says ‘the basic
principles of this organ of science are still in a state of obscurity’ [1, p. 310]. He attributes this
sorry state of affairs to two considerations.

The first is the belief that if the subject matter is susceptible to uncertainties (‘greater or smaller
errors’ [1, p. 311]) then precise definition of ideas or concepts is either impossible or unnecessary.
Of course, with the benefit of the hindsight accumulated from observation of the immense
advances that statistical tools have led to, one might nowadays state that if the subject matter
is susceptible to uncertainties then the demand for statistical methods is all the greater. Although
Rutherford had an element of truth in his famous observation that ‘if your experiment needs
statistics, you ought to have done a better experiment’, he missed the fact that any investigation
at the boundaries of knowledge must, almost by definition, have uncertainties and errors in
measurement—and hence needs statistics to tease out the answers.

Fisher secondly attributed the dire state of early twentieth century statistics to the confusion
arising from using the same word for the unknown true value of a parameter and its estimate.
Indeed, although the word ‘parameter’ was occasionally used in physics at the time, it was
virtually unknown in statistics. It was Fisher, explicitly using the terms ‘parameter’ and ‘statistic’
to distinguish between the usages in his 1922 paper, who introduced the terms and the distinction.
(According to Stigler [8], using it 57 times in the paper—I made it 58.)


From Fisher’s perspective, the key point about this second confusion is that it ‘would appear to