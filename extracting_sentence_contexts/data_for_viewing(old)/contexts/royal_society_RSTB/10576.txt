acquired from individual learning episodes, or visual input [34] demonstrated that there are two dissociable mechanisms that 2.
made sense of (cf. visual statistical learning), but from the per- afford such crose-domain transfer of knowledge: one based on = =
spective of trying to understand the relationship between the abstract analogy of ‘repetition structure’ [35], in which a sequence =
7 . . . such as ‘jix pel jix sog’ can be mapped onto ‘+ *+=" (novel symbols =
different learning and memory systems that underpin experi- were used, not the mathematical symbols shown here), and impor- 2
ence and memory, and abstraction and generalization (see tantly, another based solely on computing, and mapping between, o
[66,67] for an example of this broader perspective, in which statistical distributions of non-repeating elements (e.g. ‘jix pel het ‘©
sleep takes on a central role in statistical learning). The distinc- sog’ onto ‘+ *~=" — unlike abstract analogy, this process requires __
tion between semantic types and semantic tokens is just one statistical abstraction across multiple exemplars to induce the statisti- =
small component ina renter theoretical landscape : lated cal distributions). The model was developed specifically to explain =
Pp 1 pe popu this latter case, i. the mapping of statistical patterns in one S
by a wealth of relevant behavioural, neuroscientific and domain, onto statistical patterns in the other. =
computational data. The challenge is to abstract across it all. “It is unclear whether these episodes are, in fact, ‘temporally separ- UL,
ated’. There is, necessarily, continuity from one episode to the next, R
Competing interests. I declare I have no competing interests. and most likely their representations overlap. Temporal discontinuity S
Funding. We received no funding for this study. may manifest in their encoding as discontinuities in predictability of x
ula . . episodic state across time (cf. the computational instantiation of event NS
Acknowledgements. I thank the editors for encouraging my attempt to segmentation theory described in [47]) y
link statistical learning to broader issues in cognition. I also thank ‘The current account of tokenization differs from that developed in s

Eiling Yee, Zac Ekves and two anonymous reviewers for their excel-
lent comments on an earlier draft. Limitations of space precluded a
more rigorous review of relevant studies, of which there are many.
The ideas expressed here have developed over the course of multiple
conversations with a number of colleagues, including Zac Ekves,