matrix S. In particular, we are interested in the largest eigenvalue, 41 (and corresponding
eigenvector aj), since the eigenvalues are the variances of the linear combinations defined by
the corresponding eigenvector a: var(Xa) =a’Sa=)a’a=4. Equation (2.1) remains valid if the
eigenvectors are multiplied by —1, and so the signs of all loadings (and scores) are arbitrary and
only their relative magnitudes and sign patterns are meaningful.

Any p x p real symmetric matrix, such as a covariance matrix S, has exactly p real eigenvalues,
Ax (k=1,...,p), and their corresponding eigenvectors can be defined to form an orthonormal set
of vectors, ie. a,ay =1 if k=’ and zero otherwise. A Lagrange multipliers approach, with the
added restrictions of orthogonality of different coefficient vectors, can also be used to show that
the full set of eigenvectors of S are the solutions to the problem of obtaining up to p new linear
combinations Xa, = a kX, which successively maximize variance, subject to uncorrelatedness :
with previous linear combinations [4]. Uncorrelatedness results from the fact that the covariance :=

between two such linear combinations, Xax and Xay,, is given by aj, Sax = Agaj,ay = O if k’ Ak. Sy

It is these linear combinations Xa, that are called the principal components of the dataset, 2
although some authors confusingly also use the term ‘principal components’ when referring aed
to the eigenvectors ax. In standard PCA terminology, the elements of the eigenvectors ax are _
commonly called the PC loadings, whereas the elements of the linear combinations Xa, are called : 8
the PC scores, as they are the values that each individual would score on a given PC. DNS

It is common, in the standard approach, to define PCs as the linear combinations of the : z
centred variables x*, with generic element ij = Xij — Xj, where Xj denotes the mean value of the : Ss
