On the other hand, the third assumption is not necessary and can be relaxed in different ways,
but most of the theory makes this rather strict assumption for simplicity. So, the real fundamental departure from conventional multi-variate statistics is to assume that
the components are non-Gaussian. Non-Gaussianity also gives a new meaning to independence:
for variables with a joint Gaussian distribution, uncorrelatedness and independence are :
in fact equivalent. Only in the non-Gaussian case is independence something more than :z
uncorrelatedness. Uncorrelatedness is assumed in other methods such as PCA and factor analysis, [3
but this non-Gaussian form of independence is usually not. a

As a trivial example, consider two-dimensional data that are concentrated on four points: :8
(—1,0), (1,0), (0, —1), (0,1) with equal probability t The variables x; and x2 are uncorrelated [=
owing to symmetry with respect to the axes: if you flip the sign of x1, the distribution stays 1
the same, and thus we must have E{x,x2} = E{(—x,)x2}, which implies their correlation (and :g
covariance) must be zero.