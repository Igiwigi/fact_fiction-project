methodologies is reported in §1c. It is worth mentioning that there is still no agreement within the
ML community on the definition of the terms ‘interpretability’ and ‘explainability’ [15,16]. Even if
several authors attempted to distinguish between them, most use the two terms interchangeably.
Although a clear universal definition is still not available, for the sake of convenience, we will
refer here to the distinction reported in Guidotti et al. [15], where interpretability is defined as the
ability to explain, or to present, the reason behind the decision of an ML model in understandable
terms to a human.

In this study, we introduce two interpretability frameworks, specifically designed for DNNs
trained for the ECG classification task, that let us inspect the decision of DNNs, unveiling which
waves of the input ECG were most relevant to the final classification. In this work, we refer to the
P-wave, QRS complex and T-wave composing the ECG beat as simply ‘waves’. The rationale
behind the development of new interpretability frameworks in this context relies on the fact
that the evaluation of the interpretation itself is challenging with the current interpretability
methodologies. Indeed, such methodologies can highlight the most important samples of the ECG
contributing to the classification. However, understanding whether such samples are meaningful
for the cardiac abnormality to detect is not currently handled. For example, a single heartbeat
on 1-lead ECG sampled at 1000 Hz has approximately 600 samples, and each sample has a
weight on the classification. However, for a given cardiac abnormality, only some ECG samples
must be useful for the classification, based on prior knowledge from electrocardiography. In
order to address such an issue, we propose to combine two modules. The first one will provide
interpretability by using two state-of-the-art techniques. The second one will assess whether the


most important samples are matching those expected to be affected by the cardiac abnormality,