Any p x p real symmetric matrix, such as a covariance matrix S, has exactly p real eigenvalues,
Ax (k=1,...,p), and their corresponding eigenvectors can be defined to form an orthonormal set
of vectors, ie. a,ay =1 if k=’ and zero otherwise. A Lagrange multipliers approach, with the
added restrictions of orthogonality of different coefficient vectors, can also be used to show that
the full set of eigenvectors of S are the solutions to the problem of obtaining up to p new linear
combinations Xa, = a kX, which successively maximize variance, subject to uncorrelatedness :
with previous linear combinations [4]. Uncorrelatedness results from the fact that the covariance :=

between two such linear combinations, Xax and Xay,, is given by aj, Sax = Agaj,ay = O if k’ Ak. Sy

It is these linear combinations Xa, that are called the principal components of the dataset, 2
although some authors confusingly also use the term ‘principal components’ when referring aed
to the eigenvectors ax. In standard PCA terminology, the elements of the eigenvectors ax are _
commonly called the PC loadings, whereas the elements of the linear combinations Xa, are called : 8
the PC scores, as they are the values that each individual would score on a given PC.