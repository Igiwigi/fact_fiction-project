Aa P,(o|s,,a,)

 2,1P,(os,a,) + (1-2,_)F,

Pols,

NY learning Qa) ~ RLQ,_\(5p4,).0,)

stim.action outcome P,(o/s,,a,) ~ outcome freq.

a,) T= equiprob. of actor outcomes

Figure 1. Factual reactive inferences in the rodent prefrontal cortex. (a) Inferential system arbitrating between actor learning and creation from long-term memory.
Q, selective and P, predictive models forming behavioural strategies stored in the long-term memory repertoire (superscript f). Subscript t (trial number) indicates
actor strategy driving ongoing behaviour and learning external contingencies through reinforcement learning (RL) and action outcome frequencies. A,, actor absolute
reliability inferred online (right inset: 17, default-likelinood). t = 0, time when the actor becomes unreliable (A, < 1 — A,) and a new actor is created by mixing
stored strategies weighted according to predictive models. Green, confirmation events when the actor becomes reliable (A, > 1 — A,). (6) Presumed system
implementation in paralimbic prefrontal regions (rodents). DS, dorsal and VS, ventral striatum filtering out non-actor strategies and learning external contingencies.
PM, ACC and OFC, premotor, anterior-cingulate cortex and orbitofrontal cortex, respectively. Red, actor creation triggered in ACC; actor filtering in the striatum is off
and allows mixing strategies stored in OFC and PM. See text for explanation.

relation to the premotor cortex processes stimulus—response or simply the actor), the prefrontal cortex may have evolved
associations guiding action selection [13,16-18]. as arbitrating online between these two options: (i) staying

However, RL has severe adaptive limitations. The most evi- with the current actor strategy, which adjusts through RL to