remaining alternative ways of thinking, one that inherently tended to lead to a focus on the
joint distribution of the given data as opposed to the (hypothesized) generator of individual
observations. Now almost all statistical methods follow Quetelet and privilege as the theoretically
crucial entity the (conditional) mean—the variation around the prediction is simply an error
distribution often chosen for reasons of mathematical convenience or empirical fit.

The second curse has been computing power. It has become nearly trivial to estimate more or
less any mildly identified model that one can write down, and hence there has been an explosion
of terms on the right-hand side. The result is not only thousands of incomparable ‘models’ that
really are bereft of any strong interpretation, but a mistaken vision of science as progressing
by increasing the number of concrete determinants—as if Galileo could be criticized for only
thinking about point masses, and not also including balloons, feathers and magnets when he did
his (fictional) experiments [49]. This has led us to prefer models that seem to possess realism,
but interfere with our capacity to understand more elegant, and more theoretically generative,
simplifications that might speak to the nature of distributions.


Consider the theoretical clarification that came after Quetelet’s development of the statistics
for social phenomena. He was struck by the fact that a predictable mathematical shape arose
in a population. He had made a reasonable stab at explaining the compatibility of this putative
lawfulness and free will, but his ideas could not but give rise to the notion, put forward by his
popularizer Henry Buckle, that, e.g. a certain number of people had to kill themselves, and if one
resisted the dark urges, another would find them all the more pressing. Even Emile Durkheim
was not free from this interpretation.
