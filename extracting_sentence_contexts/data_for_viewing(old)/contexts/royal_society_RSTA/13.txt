means that the sum of squared distances between corresponding points in each scatterplot is

minimized, as in the original approach by Pearson [1]. The system of q axes in this representation : &
is given by the first q PCs and defines a principal subspace. Hence, PCA is at heart a dimensionality- Dos
reduction method, whereby a set of p original variables can be replaced by an optimal set of : =
q derived variables, the PCs. When q=2 or q=3, a graphical approximation of the n-point : yy
scatterplot is possible and is frequently used for an initial visual representation of the full dataset. 2
It is important to note that this result is incremental (hence adaptive) in its dimensions, in the : S
sense that the best subspace of dimension q+ 1 is obtained by adding a further column of 22
coordinates to those that defined the best q-dimensional solution. : 8

The quality of any q-dimensional approximation can be measured by the variability associated : ~
with the set of retained PCs. In fact, the sum of variances of the p original variables is the trace :B
(sum of diagonal elements) of the covariance matrix S. Using simple matrix theory results it is : Ss

straightforward to show that this value is also the sum of the variances of all p PCs. Hence, the
standard measure of quality of a given PC is the proportion of total variance that it accounts for,
4j ii
Tj aa yj iS)â€™ (2.6)
where tr(S) denotes the trace of S. The incremental nature of PCs also means that we can speak
of a proportion of total variance explained by a set S of PCs (usually, but not necessarily, the first
q PCs), which is often expressed as a percentage of total variance accounted for: Nie s MH X 100%.

It is common practice to use some predefined percentage of total variance explained to decide
how many PCs should be retained (70% of total variability is a common, if subjective, cut-off