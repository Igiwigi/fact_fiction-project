on validation simulations Yy or CoU simulations Yp can be computed by propagating the
uncertainty through the model f in the validation/CoU simulations, e.g.

x(¥P| Ye) =| 20% | 0)x(0 | Yc)d.

This is referred to as ‘uncertainty propagation’ or the ‘posterior predictive distribution’.
Uncertainty in the prediction of Yy helps provide a more informed comparison to the observed
validation data (especially if experimental error in Yy is also accounted for). Uncertainty in Yp
enables a more informed model-based decision-making process.

(c) Model discrepancy

UQ as outlined above does not account for the fact that the model is always an imperfect
representation of reality, due to limited understanding of the true data-generating mechanism and
perhaps also any premeditated abstraction of the system. The model discrepancy is the difference
between the model and the true data-generating mechanism, and its existence has implications
for model selection, calibration and validation, and CoU simulations.

For calibration, the existence of model discrepancy can change the meaning of the estimated
parameters. If we fail to account for the model discrepancy in our inference, our parameter
estimates, instead of being physically meaningful quantities, will have their meaning intimately
tied to the model used to estimate them (we end up estimating ‘pseudo-true’ values; see §2c). The
estimated parameter values depend on the chosen model form, and the uncertainty estimates
obtained during inverse parameter UQ tell us nothing about where the true value is (only how
confident we are about the pseudo-true values). In other words, there is no guarantee the obtained