{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets contexts for each keyword of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "class TextContextExtractor:\n",
    "    def __init__(self, keywords, sentences_before, sentences_after):\n",
    "        self.keywords = [kw.strip() for kw in keywords]\n",
    "        self.sentences_before = sentences_before #how many sentences before are counted\n",
    "        self.sentences_after = sentences_after #how many sentences after are counted\n",
    "\n",
    "    def extract_text_from_txt(self, txt_path):\n",
    "        \"\"\"Extract text from a given TXT file.\"\"\"\n",
    "        with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def find_contexts(self, text):\n",
    "        \"\"\"\n",
    "        Extracts contextual snippets around specified keywords within a text. \n",
    "\n",
    "        How it works:\n",
    "        1. Splits the input text into sentences by looking for punctuation (., !, ?) \n",
    "        followed by a space and a capital letter (indicating the start of a new sentence).\n",
    "        2. Iterates over each keyword defined in keywords.\n",
    "        3. For each keyword, searches all sentences for matches (case-insensitive, \n",
    "        and matching whole words only to avoid partial matches).\n",
    "        4. When a keyword is found, selects a range of sentences around it:\n",
    "        - `sentences_before` sentences before the keyword sentence\n",
    "        - `sentences_after` sentences after the keyword sentence\n",
    "        5. Joins these sentences together into a single context snippet and stores \n",
    "        it along with the keyword.\n",
    "        \n",
    "        Returns:\n",
    "            A list of tuples: [(keyword1, context1), (keyword2, context2), ...]\n",
    "            Each tuple contains the keyword and the extracted context surrounding it.\n",
    "        \"\"\"\n",
    "\n",
    "        contexts = []\n",
    "        sentence_pattern = r'(?<=[.!?])\\s+(?=[A-Z])' #pattern of sentences, question marks and capitalised letters\n",
    "        sentences = re.split(sentence_pattern, text)\n",
    "        sentences = [sent.strip() for sent in sentences if sent.strip()]\n",
    "        \n",
    "        for keyword in self.keywords:\n",
    "            keyword_pattern = r'\\b' + re.escape(keyword) + r'\\b' #word boundary + special char won't break regex\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                if re.search(keyword_pattern, sentence, flags=re.IGNORECASE):\n",
    "                    start_idx = max(0, i - self.sentences_before)\n",
    "                    end_idx = min(len(sentences), i + 1 + self.sentences_after)\n",
    "                    context_sentences = sentences[start_idx:end_idx]\n",
    "                    context = ' '.join(context_sentences)\n",
    "                    contexts.append((keyword, context))\n",
    "        return contexts\n",
    "\n",
    "    def save_contexts_to_csv(self, contexts, output_path):\n",
    "        \"\"\"Save extracted contexts to a CSV file.\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Filename\", \"Keyword\", \"Context\", \"Author\", \"Title\", \"Date\"])\n",
    "            writer.writerows(contexts)\n",
    "\n",
    "    def convert_csv_to_excel(self, input_csv, output_excel):\n",
    "        \"\"\"\n",
    "        Convert a CSV file to an Excel file, ensuring that context text is formatted as plain text in Excel.\n",
    "       \n",
    "        Parameters:\n",
    "        - input_csv (str): Path to the input CSV file.\n",
    "        - output_excel (str): Path to the output Excel file.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(input_csv)\n",
    "       \n",
    "        def format_as_text(value):\n",
    "            \"\"\"Ensure that the text is formatted as plain text in Excel.\"\"\"\n",
    "            if isinstance(value, str) and value.startswith('='):\n",
    "                return f\"'{value}\"\n",
    "            return value\n",
    "        \n",
    "        df['Context'] = df['Context'].apply(format_as_text)\n",
    "        \n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(os.path.dirname(output_excel), exist_ok=True)\n",
    "       \n",
    "        df.to_excel(output_excel, index=False, engine='xlsxwriter')\n",
    "       \n",
    "\n",
    "    def load_metadata_csv(self, metadata_path):\n",
    "        \"\"\"Load metadata from a CSV file into a dictionary.\"\"\"\n",
    "        metadata = {}\n",
    "        with open(metadata_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                if 'filename' in row:\n",
    "                    file_location = row['filename']\n",
    "                else:\n",
    "                    file_location = row.get('pdf_link', '').split('/')[-1]\n",
    "                metadata[file_location] = row\n",
    "        return metadata\n",
    "\n",
    "    def process_texts(self, folder_path, output_csv, metadata_csv, create_excel=True):\n",
    "        \"\"\"\n",
    "        Extract keyword contexts from all text files in a folder, enrich them with \n",
    "        metadata (extracted from data before in R), and save to a CSV. \n",
    "        Optionally converts the CSV to Excel.\n",
    "\n",
    "        Steps:\n",
    "        1. Loads metadata from a CSV file into a dictionary.\n",
    "        2. Iterates through each TXT file in `folder_path`.\n",
    "        3. Extracts text, finds contexts around keywords, and combines with metadata \n",
    "        (author, title, date).\n",
    "        4. Saves all contexts to `output_csv`.\n",
    "        5. Optionally creates an Excel version in a subfolder 'excel_ver'.\n",
    "\n",
    "        Args:\n",
    "            folder_path (str): Folder containing text files to process.\n",
    "            output_csv (str): Path to save the combined contexts CSV.\n",
    "            metadata_csv (str): Path to CSV containing metadata for the files.\n",
    "            create_excel (bool, default=True): If True, also creates an Excel file.\n",
    "        \"\"\"\n",
    "        metadata = self.load_metadata_csv(metadata_csv)\n",
    "        all_contexts = []\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                txt_path = os.path.join(folder_path, filename)\n",
    "                text = self.extract_text_from_txt(txt_path)\n",
    "                contexts = self.find_contexts(text)\n",
    "\n",
    "                for keyword, context in contexts:\n",
    "                    pdf_filename = os.path.splitext(filename)[0] + \".pdf\"\n",
    "                    txt_filename = filename\n",
    "                \n",
    "                    #messy way of getting the pdf_filename between diff types of metadata\n",
    "                    #could be improved but works for now\n",
    "                    if pdf_filename in metadata:\n",
    "                        file_metadata = metadata[pdf_filename]\n",
    "                    elif txt_filename in metadata:\n",
    "                        file_metadata = metadata[txt_filename]\n",
    "                    elif 'filename' in metadata and txt_filename in metadata['filename']:\n",
    "                        file_metadata = metadata['filename'][txt_filename]\n",
    "                    else:\n",
    "                        print(f\"No metadata found for {filename}\")\n",
    "                        continue\n",
    "\n",
    "                    author = file_metadata.get('author', '')\n",
    "                    title = file_metadata.get('title', '')\n",
    "                    date = file_metadata.get('date', '')\n",
    "\n",
    "                    all_contexts.append((filename, keyword, context, author, title, date))\n",
    "\n",
    "        self.save_contexts_to_csv(all_contexts, output_csv) #might not need both csv and excel\n",
    "        \n",
    "        # Optionally create Excel file in excel_ver subfolder (easier to work with/possibly read)\n",
    "        # should be a separate function\n",
    "        if create_excel:\n",
    "            csv_dir = os.path.dirname(output_csv)\n",
    "            csv_basename = os.path.splitext(os.path.basename(output_csv))[0]\n",
    "            excel_dir = os.path.join(csv_dir, 'excel_ver')\n",
    "            output_excel = os.path.join(excel_dir, csv_basename + '.xlsx') #one folder down from the csv\n",
    "            #output_excel = os.path.join(csv_basename + '.xlsx') #if in same folder\n",
    "\n",
    "            self.convert_csv_to_excel(output_csv, output_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"fact\", \"fiction\", \"facts\", \"facſ\", \"fictions\", \"ficſions\", \"factual\", \"fictional\", \"fictionally\", \"factually\", \"fictionality\", \"factuality\", \"fictionalized\", \"factualized\", \"fictive\", \"factive\", \"fictitious\", \"factious\"]\n",
    "\n",
    "extractor = TextContextExtractor(\n",
    "    keywords=keywords,\n",
    "    sentences_before=2,\n",
    "    sentences_after=2,\n",
    ")\n",
    "\n",
    "#RSTA (#royal society)\n",
    "extractor.process_texts(\n",
    "    folder_path=\"D:/Fact_fiction_corpus/texts/royal society/txt_rsta\",\n",
    "    output_csv=\"../data_to_view/contexts_all_together/contexts_RSTA.csv\",\n",
    "    metadata_csv=\"D:/Fact_fiction_corpus/texts/royal society/royalsociety_metadata_rsta.csv\",\n",
    "    create_excel=True\n",
    ")\n",
    "\n",
    "#RSTB (#royal society)\n",
    "extractor.process_texts(\n",
    "    folder_path=\"D:/Fact_fiction_corpus/texts/royal society/txt_rstb\",\n",
    "    output_csv=\"../data_to_view/contexts_all_together/contexts_RSTB.csv\",\n",
    "    metadata_csv=\"D:/Fact_fiction_corpus/texts/royal society/royalsociety_metadata_rstb.csv\",\n",
    "    create_excel=True\n",
    ")\n",
    "\n",
    "#RSTL (#royal society)\n",
    "extractor.process_texts(\n",
    "    folder_path=\"D:/Fact_fiction_corpus/texts/royal society/txt_rstl\",\n",
    "    output_csv=\"../data_to_view/contexts_all_together/contexts_RSTL.csv\",\n",
    "    metadata_csv=\"D:/Fact_fiction_corpus/texts/royal society/royalsociety_metadata_rstl.csv\",\n",
    "    create_excel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = TextContextExtractor(\n",
    "    keywords=keywords,\n",
    "    sentences_before=2,\n",
    "    sentences_after=2,\n",
    ")\n",
    "\n",
    "#General Magazine of Arts and Sciences (albeit empty)\n",
    "extractor.process_texts(\n",
    "    folder_path=\"D:/Fact_fiction_corpus/texts/General Magazine of Arts and Sciences/txt\",\n",
    "    output_csv=\"../data_to_view/contexts_all_together/contexts_general_magazine.csv\",\n",
    "    metadata_csv=\"D:/Fact_fiction_corpus/texts/General Magazine of Arts and Sciences/general_magazine_metadata.csv\",\n",
    "    create_excel=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = TextContextExtractor(\n",
    "    keywords=keywords,\n",
    "    sentences_before=2,\n",
    "    sentences_after=2,\n",
    ")\n",
    "\n",
    "#The Spectator\n",
    "extractor.process_texts(\n",
    "    folder_path=\"D:/Fact_fiction_corpus/texts/spectator/txt\",\n",
    "    output_csv=\"../data_to_view/contexts_all_together/contexts_spectator.csv\",\n",
    "    metadata_csv=\"D:/Fact_fiction_corpus/texts/spectator/spectator_metadata.csv\",\n",
    "    create_excel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data_to_view/contexts_all_together/extracted_contexts_from_spectator.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#testing the output txt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data_to_view/contexts_all_together/extracted_contexts_from_spectator.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilename: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFilename\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data_to_view/contexts_all_together/extracted_contexts_from_spectator.csv'"
     ]
    }
   ],
   "source": [
    "#testing the output txt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data_to_view/contexts_all_together/extracted_contexts_from_spectator.csv\")\n",
    "\n",
    "for index, row in df.head(10).iterrows():\n",
    "    print(f\"Filename: {row['Filename']}\")\n",
    "    print(f\"Keyword: {row['Keyword']}\")\n",
    "    print(f\"Author: {row['Author']}\")\n",
    "    print(f\"Title: {row['Title']}\")\n",
    "    print(f\"Context: {row['Context']}\")\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
