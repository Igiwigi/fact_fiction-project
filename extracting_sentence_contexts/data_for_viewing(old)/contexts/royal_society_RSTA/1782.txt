Figure 2. State diagram for the Tsetlin automaton.

In a single-class inference problem, the output layer is a simple thresholding function. If the
votes are positive (or zero), the input data are determined to belong to the class. For a negative
sum, the input data are determined to not be in the class. For multi-class problems, we replace
thresholding with argmax to determine the output class (D, figure 1). In this case, the class
summation becomes indicative of confidence and argmax chooses the class with the highest
confidence, thus avoiding any ambiguity in classification.

(b) Reinforcement

Fundamental to reinforcement are the team of Tsetlin automata (C, figure 1). Such automata are
also known as automata with linear tactics to emphasize the fact that they allowed gradual ascent,
or reinforcement, in performing a particular action, and equally gradual descent from one action
to performing another action. A variety of types of such learning automata have been studied
in [21].

In the Tsetlin machine implementation, a two-action Tsetlin automaton is described by the
state diagram in figure 2. The automaton may be given a reward, causing it to reinforce the
current action decision (e.g. action 1) by moving away from the midstate (i.e. state n in figure 2).
Conversely it may be given a penalty, which moves the state towards the decision boundary.

In relation to processing the binarized literal through Tsetlin automata within clauses, the
two actions are include and exclude. The update of the automata requires reinforcement through
