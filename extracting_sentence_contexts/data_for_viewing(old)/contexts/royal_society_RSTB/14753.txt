arranged in a custom matrix providing a total of 58 acquisition [79], here facial expressions are partitioned into discrete muscular
channels per participant. For consistency, the placement of the components and dynamics without association with emotional
most anterior midline optode holder on the cap was centred labels. Participants rated overall affect intensity and valence
one channel length above nasion. To ensure acceptable signal- rather than naming an emotion associated with the movie clip
to-noise ratios, intensity was measured for each channel before (Movie Watcher) or facial expression (Face Watcher). The facial
recording, and adjustments were made for each channel until AU analysis using OpenFace included 17 separate classifications
all optodes were calibrated and able to sense known quantities of anatomical configurations.
of light from each laser wavelength [61,65,66]. Anatomical Conventional methods to validate the relationship between
locations of optodes in relation to standard head landmarks emotions and facial expressions have employed manual codes
were determined for each participant using a structure.io three- [80,81]. For example, Ekman, Friesen & Hager developed a
dimensional scanner (Occipital, Boulder, CO) and portions of manual observer-based method for coding facial expression
code from the fieldtrip toolbox implemented in Matlab 2022a measurements [82] referred to as the facial action coding
[67-71]. Optode locations were used to calculate positions of system (FACS). FACS provides a technique to record an objective
recording channels (figure 1b), and Montreal Neurological Insti- description of facial expressions based on activations of facial
tute (MNI) coordinates [72] for each channel were obtained with muscles and has provided a foundation to link human emotions
NIRS-SPM software [51] and WFU PickAtlas [73,74]. with specific human facial expressions. By contrast, application

of the OpenFace platform in this investigation does not relate
facial AUs to any specific emotion. Spontaneous expressions of

(g) Eye-tracking the Movie Watcher are classified as discrete constellations of
Two Tobii Pro x3-120 eye trackers (Tobii Pro, Stockholm, Sweden), moving parts (AUs) and rated by the Face Watcher using a
one per participant, were used to acquire simultaneous eye-track- scale from â€”5 to +5 indicating affect valence and intensity.
ing data at a sampling rate of 120 Hz. Eye trackers were mounted There is no inference with respect to a specific emotion.
