:

Ss

waveform is not overly distorted or corrupted by noise, and if the waveforms in the codebook are : =
reasonably well separated in relation to the distortion, then the receiver’s guess will be correct, : g
producing i =m. 2
The channel capacity in bit/s can now be mathematically defined as follows. We first decide : s
ona given non-zero error probability threshold € that can be accepted and a receiver observation : 3

duration T. Then, we search for the largest codebook such that an ideal receiver, upon observing
y(t) over the interval 0 <t<T, can correctly guess the transmitted message m with probability
no less than 1 — e. Disregarding the facts that this search process may be prohibitively complex
and that the ideal receiver may be unknown, the size of this optimal codebook is denoted by M*
and the corresponding rate by R = (logy M*)/T. This quantity is the maximum achievable rate at
duration T and error probability €, and it exists theoretically for any T > 0 and 0 <« <1, even if
we usually do not know how to compute it.

Next, we let T increase, still for a given non-zero €, which means that M* also increases.
Depending on the type of channel and the value of €, the ratio R may increase or decrease with
increasing T, but in any case it converges to a limit, C = limr_, ., R. This limit defines the capacity
of the channel. One might expect that C would decrease as € decreases, but the limit is the same,
regardless of €, as long as it is not 0 or 1 [2, theorem 12]. The qualitative behaviour of R is well
represented by figure 2 in §2b, if n is replaced by T, for more or less any channel. A threshold
phenomenon is evident at high T: every rate below capacity is achievable even at very low error