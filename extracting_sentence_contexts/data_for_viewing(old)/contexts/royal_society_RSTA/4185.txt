FTPs, owing to Fuchs & Schack [2] and my own [10], lead to the same result.

One can pose a deeper question: Is one of these generalized FTPs really realized by brain’s
‘hardware’ to perform DM and PU? For the moment, this is a very speculative question. But who
knows?

2. Subjectivist model of learning: classical (Boolean) probability framework

Here, we use some parts of the presentation from [58] on the subjectivist approach to PU and
learning. We complete this presentation with remarks on the role of CP and Boolean logic. The
probability of a hypothesis H conditional on a collected data E is the ratio of the unconditional
probability of the conjunction of the hypothesis with the data to the unconditional probability of
the data alone. This is the famous Bayes’ formula (in fact, serving as the definition of conditional
probability):
p(H&E)
ple)
provided that both terms of this ratio exist and p(E) > 0.
Bayes’ theorem relates the ‘direct’ probability of a hypothesis conditional on the data, p(H|E),
to the ‘inverse’ probability of the data conditional on the hypothesis, p(E|H),
pA)

p(HIE) = Fal p(E\H). (2.2)

This possibility to ‘invert’ probability p(H|E), ie. express it through the probability p(E|H) is based
on the use of Boolean logic, the basis to the modern probability theory and Bayesian reasoning. It