showing that both native and non-native phonetic presented in quiet with little reverberation, speech
perception skills of infants predict their later language communication in everyday life often takes place in the
ability, but in opposite directions. Better native- presence of background sounds and reverberation. The
language skill at seven months predicts faster language issues raised by this are considered in the paper by
advancement, whereas better non-native-language skill Darwin (2008). He points out that irrelevant background
predicts slower advancement. Kuhl et al. suggest that sounds can cause severe problems for computer-based
native-language phonetic performance is indicative of | speech recognition algorithms and for people with
commitment of neural circuitry to the native language, hearing impairment, but that people with normal hearing
while non-native phonetic performance reveals uncom- are remarkably little affected. A variety of perceptual
mitted neural circuitry. This paper describes a revised problems are created by the presence of background
version of a model previously proposed by Kuhl and sounds. These include: complete or partial masking of
co-workers, the native language magnet model. some parts of the target speech; the need to decide which
The paper by Campbell (2008) emphasizes the fact parts of the sound ‘belong to’ each sound source; and the
that speech perception is multimodal; what we perceive recognition of speech sounds based on partial infor-
as speech is influenced by what we see on the face of the mation. Darwin examines the effectiveness of the cues,
talker as well as by what is received at the two ears. This which can be used to separate target speech from a
is illustrated by the McGurk effect (McGurk & background of other sounds (including competing
MacDonald 1976), which is produced when a video speech), focusing particularly on the role of fundamental
recording of one utterance is combined with an audio frequency, onset asynchronies and binaural cues. At
recording of another utterance. What is heard is present, human listeners perform far better than any
influenced by what is seen. For example, an acoustic computer system in separating mixtures of sounds.

‘mama’ paired with a video ‘tata’ is heard as ‘nana’. A fuller understanding of how humans do this would
The influence of vision on speech perception is also have important practical applications.
