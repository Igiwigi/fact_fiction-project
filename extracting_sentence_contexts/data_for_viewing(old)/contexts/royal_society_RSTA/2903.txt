user (e.g. if individual claims are likely to be small and manageable in aggregate) or covered
by insurance, producers of machine learning technology will be incentivized by this to build
explanation into their technologies so as to meet user demand. It therefore seems safe to delay
regulation until the real magnitude of the risk becomes clearer, at which point it should also be
easier to draw a line between regulated and unregulated activities.

4. Identifiable risks to the legal allocation of responsibility

Every society needs a scheme to allocate responsibility, and thus liability, for activities which cause
loss or damage to others. It does so in respect of persons (including companies), not in respect of
technologies. This is because the levers which the law uses to control or influence behaviour, such
as awarding compensation or imposing fines or imprisonment, can only be applied to persons.
Law and regulation which appears to be about technology is in fact always about the behaviour
of persons responsible for that technology.

Some areas where AI technologies are likely to prove useful have tailored responsibility
schemes, for instance, nuclear power [12], and here it will be up to the legislator or regulator
to modify the scheme appropriately. But most activities fall within the general scheme of
liability law.

In a minority of cases, the person responsible for the activity is made strictly liable, which
means that they must compensate those who suffer loss or damage, regardless of whether the
person responsible was careless when undertaking the activity. Strict liability under English law
has been a piecemeal response to the recognition of dangerous activities or states of affairs against
the consequences of which the person responsible is required to indemnify the remainder of