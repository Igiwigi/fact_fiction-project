So, in its acategorical form, I hypothesize that the
weights for each feature correspond to the average weight
given all the contexts of use of the word. In NLP the meaning of words are routinely represented as
word embeddings (e.g. [5-8]). These are vectors created from the
statistical co-occurrence of words, capitalizing on the fact that
words that mean something similar often occur in similar con-
texts.> They provide a powerful way of representing meaning
and obey simple geometric transformations (for example, sub-
tracting man from kifig and adding womiin results in a vector
that closely approximates qu√©en [74]). These vectors are typi-

The discussion so far has reviewed the literature regarding
three stages of morphological processing. While neuro-
biological research provides quite a comprehensive explanation
of how the brain segments sensory input into morphological
constituents, our understanding remains poorly defined in
terms of (i) what linguistic features make up the representation
of each morphological unit; (ii) what operations are applied to
those features at each stage of composition.