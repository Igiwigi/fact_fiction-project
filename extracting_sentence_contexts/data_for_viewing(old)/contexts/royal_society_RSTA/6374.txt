
particularly remarkable for their ability to generate fluent natural language text and dialogue
at a level that is often indistinguishable from a human. However, despite many informal
(and frequently overstated) claims about ‘human-like’ or ‘human-level’ language abilities [5],
it remains unclear whether these impressive engineering achievements can offer insights to the
study of language and cognition more broadly. The goal of this article is to present a case that they
can: that is, that neural networks (LLMs specifically) can serve as plausible models of language
understanding in humans.

I focus on two common criticisms of LLMs: (i) their lack of symbolic structure (§2), a phrase
I use broadly to encompass a collection of criticisms including sample inefficiency, poor out of
distribution generalization, lack of compositionality and inability to reason logically, and (ii) their
lack of grounding (§3), i.e. the fact that they are trained only on text and thus have no access
to or awareness of the physical, perceptual, goal-oriented or social contexts in which language
occurs. Both of these criticisms are typically supported by empirical data demonstrating that
LLMs perform extremely poorly on tasks that appear to require symbols [6-9] or grounding
[10,11], respectively.

My counterargument is based on a premise that is commonplace in cognitive science:
performance is not the same as competence, and analysing the performance of a system alone
offers only limited insight into the system’s underlying competence.! Until we can precisely
characterize the representations and mechanisms in play under the hood, examples of LLMs’
behavioural successes or failures tell us little about LLMs’ ability to serve as models of language
in humans. Of course, it can be argued that requiring analysis of the internal processing of
LLMs amounts to holding LLMs to a higher bar than that to which we hold humans. We