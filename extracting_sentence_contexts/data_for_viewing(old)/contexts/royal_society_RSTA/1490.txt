Figure 2. Traces of activations of reservoir nodes over learning time for Lorenz 63. Top: Activation traces for a set of randomly
selected reservoir nodes. Bottom: Evolution of the first state component through time (x), black) and reconstruction of that
state by linear combination (%, computed estimated via W,,:) of all the reservoir traces (red). The reconstruction is essentially
perfect, with the red line coinciding exactly with the black one. (Online version in colour.)

(ii) Reservoir must be sufficiently large for good generalization performance

We begin by evaluating the impact of the reservoir size N on the system’s performance on the
L63 task. N controls the number of features available for the linear regression used to solve for
Wout, Where the features come from the projected inputs Winx, further compounded with A every
update. Thus, we expect that below a critical N the span of the features will be insufficient to
fit the dynamics at all, with the span of the features saturating with large N (as all features
originally come from inputs x). This is in fact exactly what we see in figure 3a: performance
quickly improves up until about N = 50, and then improvement asymptotes as N continues to get
larger.

(iii) Reservoir unit nonlinearity o is not necessary for good generalization

Next, we examine the effect of changing the unit activation function o = tanh which is applied
to units at every update. The activation function (compounded throughout each time step)
is supposed to provide a critical nonlinearity to the reservoir’s features. This nonlinearity is
proposed to be the key reason why ESNs are able to model a wide variety of systems well.
However, we discovered that (figure 3b), in our parameter regime, removing the nonlinearity
entirely had only a minor (but positive) effect on overall performance!
