be relevant to decision problem 1, while knowing — gtochastic model of evolution, with either a growing
the actual element of partition B may be relevant —_ population or one whose size is fixed at some carrying
to decision problem 2. This opens up the possibility capacity (Shreiber 2001; Benaim et al. 2004; Taylor
of signalling dialogue, where information flows in — ¢ gj. 2004). Pawlowitsch (2007) has shown that in one
two directions kind of finite population model, efficient proto-
languages are the only strategies that are protected by
selection. Individuals might interact with neighbours in
In the simplest sort of example, nature flips a coin some spatial structure (Grim et al. 2002; Zollman
and presents player 2 with one or another decision 2005). Isolated individuals might invent signalling
problem. Player 2 sends one of two signals to player 1. systems by trial-and-error learning in repeated
Player 1 selects one of two partitions of the state of | interactions (Skyrms 2004, 2008; Barrett 2006,
nature to observe. Nature flips a coin and presents 2007a,b), which might then spread by a process of
player 1 with the true state. Player 1 sends one of two cultural evolution (Komarova & Niyogi 2004). In fact,
signals to player 2. Player 2 chooses one of two acts. urn models of reinforcement learning are very close to
Suppose that there are four states, {S1, S2, S3, S4}, those in a small, growing population (Shreiber 2001;
with alternative partitions: P1= {{S1, S2}, {S3, S4}}, Benaim et al. 2004). It has been recently proved that
P2={{S1, S3}, {S2, S4}}. The two decision problems reinforcement dynamics in the simplest Lewis signal-
require choices in different act sets: D1= {Al, A2}, ling game—2X2X2 states equiprobable—converges
D2= {A3, A4}. Pay-offs for the two decision problems with probability 1 to a signalling system (Argiento et al.
are shown in table 6. in press). Such an analytic treatment of reinforcement
Player 2 has a signal set {R, G} and player 1 has a learning does not yet exist for more complicated
signal set {B, Y}. A strategy for player 2 now consists of signalling interactions, but simulations tend to
three functions: a sender strategy from {P1, P2} into give results parallel to the evolutionary analysis
{R, G}; a receiver strategy form {B,Y} into {Al, A2}; given here. This is not entirely surprising, given
and a receiver strategy from {B, Y} into {A3, A4}. Ina the close connections between reinforcement learning