as black boxes

Measuring error rates from experts will provide needed indi-
cators for quality but are not the panacea. The problem is that
people (including many scientists) transpose a practice-wide
‘rate’ to any individual examiner in a given case. This is
unfair in either sense, unjustifiably penalizing for the good
examiners and unjustifiably praising for the bad examiners.
The paper by Edmond et al. [60] offers, in my view, no
viable solution for this, because it takes a practice-wide
‘rate’ for a constructive piece of information in the individual
case. For example, the study by Ulery et al. [35] reported a
rate of false positive is 0.1%. I cannot foresee how the fact
finder will consider an average error rate when an expert tes-
tifies to an opinion of identification. If nothing in the case
indicates that the expert deviated from the standard practice,
his or her opinion will be simply trusted. It means that that
claim that the ‘probability of an error considered as so
small as to be dismissed’ will be simply admitted without
any further reference to the average value of 0.1%. That pro-
cess offers no mechanism to effectively measure the actual
weight to be attached to the forensic findings. Only struc-
tured and systematic research on the features themselves
can lead to such a state. That leads me to the next topic.
