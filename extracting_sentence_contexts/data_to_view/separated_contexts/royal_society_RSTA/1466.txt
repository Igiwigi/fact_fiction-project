For example, in [17, pp. 8-9] (see also [37]) it is stated that

Soon after the introduction of «(A) for error analysis, Hestenes and Stiefel [2] showed that
this quantity also played a role in complexity analysis. More precisely, they showed that the
number of iterations of the conjugate gradient method (assuming infinite precision) needed
to ensure that the current approximation to the solution of a linear system attained a given
accuracy is proportional to //«(A). This follows from (3.4), which is not presented by Hestenes and Stiefel; in fact, it was clear
through the connections they make to orthogonal polynomials, Gauss quadrature and continued
fractions (see §2 and [2, Sections 14—18]) that the nonlinearity of CG was well understood. In the
context of solving large-scale linear systems, Chebyshev polynomials were considered in the early
works of Young, Lanczos, Rutishauser and others; see [4, Section 5.5.3] for a detailed description. However, these authors never had in mind that the upper bound for CG that can be derived using
Chebyshev polynomials should be equated with a linear ‘rate of convergence’ of CG; the concept
of a linear ‘rate of convergence’ is applicable to stationary iterative methods but makes little sense
in the context of Krylov subspace methods.