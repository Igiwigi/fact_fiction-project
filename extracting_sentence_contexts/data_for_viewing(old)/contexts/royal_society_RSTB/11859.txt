rewards are internally derived from a separate learning process.
A succinct representation of this is the Curiosity Loop, which is
composed of a Learner and an RL module ([18,19], figure 1,
left). The Learner attempts to learn some form of sensorimotor
correlation, e.g. internal models, which are influenced by the
actions selected by the agent. The intrinsic rewards are thus
proportional to the learning progress of the Learner, ie. the
more the Learner learns to approximate the appropriate corre-
lations, the higher the rewards. The RL component receives

The term ‘embodied curiosity’ refers to the application of
curiosity-based computational models within an embodied,
physical agent. It hints to the fact that there is a very tight
interaction between the computational model and the body,
ie. sensors, motors and their physical arrangement, as well
as the embedding interaction. One interaction cannot be ana-
lysed without the other, since emerging behaviours are
dependent on both. In this section, these two concepts are
briefly described and reviewed.

(a) Artificial curiosity these intrinsic rewards and changes the policy, based on the
Artificial curiosity refers to the computational models wherein RL algorithm. The agent ‘learns (RL) how to behave (policy)
the goal of the agent is learning, such that it selects its action to in order to learn (Learner)’, hence the Curiosity Loop. The
maximize learning. The models are inspired by cognitive psy- end result of the Curiosity Loop is the convergent policy,
