Moreover, hierarchical forms of active inference
as applications of the LA principle [119-122]. If the edge have been proposed as models of specific forms of cognition
weights of an ANN represent an imaginary particle’s coordi- such as associative learning, homeostasis and allostasis =
nates, then the loss function (defined as the error between the [125,126]. >
observed output and the target) would represent the poten- Language—arguably the epitome of cognition—has a __
tial energy of the system of particles and the derivatives of unique statistical signature captured by ‘Zipf’s Law’, which = =:
the error with respect to the weights would represent the kin- is a power-law distribution, which expresses the empirical — 5'
etic energies of the particles [120,121]. Thus, learning in the fact that the frequency of a word is inversely proportional = =
neural network involves transforming the potential energy to its rank (the position of the word in a list ordered by fre- =
(error) into kinetic energies (weight alteration), together con- quency) [127]. For example, the most frequent word occurs =
stituting the ‘cognitive energy’. The principle of LA simply about twice as frequently as the second most frequent iat
helps compute the ‘cognitive action’ (the learning trajectory), word.