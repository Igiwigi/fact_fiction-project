the technology fails to make a correct decision. fa

And, a regulator who is deciding, for example, whether to licence some AI technology for :
general use will depend on the producer to provide some, but not all, of the information the Ss
producer can generate, but more importantly will need the producer to explain the meaning Dw
of that information and its implication for use of the technology. This is what Zarsky [26] 1m
terms ‘interpretability’; disclosing an algorithm, for example, will probably only convey meaning a
to a technologist working in the field, so any explanation will need to translate what the aes
AI is doing in terms which are meaningful to the recipient of the explanation. Such an .
explanation might even make no attempt to explain the algorithm at any level of abstraction,
if doing so would not achieve the purpose for which the explanation is required. Wachter
et al. [27] point out that what they describe as ‘counterfactuals’, examples of changes to the
facts which would have produced a different decision by the AI, can be more illuminating
to some recipients if the aim is to enable those recipients to understand why a decision
was made.

Any legal requirement to incorporate transparency into AI needs to take account of these
differences in perspective. Merely demanding transparency is meaningless without the context
of the human who requires transparency. It is therefore essential to define a requirement for
transparency in terms of the human who needs to understand the decision-making processes
of the AI in question.

We also need to ask what purpose the understanding is to serve. The UK House of Lords
Artificial Intelligence Committee has recently suggested that in order to achieve trust in AI tools
