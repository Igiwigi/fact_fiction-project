For example, we showed earlier the out-of-bag coverage of our trained QRF (figure 3). Metrics such as the CRPS, logarithmic score, and Kullbackâ€”Leibler divergence would provide
good comparisons of forecast skill on which to base quantile averaging weight, although their
calculation would add some additional processing time. Yao et al. [40] provide more detail about
using such metrics for weighted model stacking, and in fact these weights can be optimized as an
additional supervised learning problem [33]. The overall strategy for combining forecasts is also open to further research. Because it retains
the inter-model variance, BMA may be considered to provide a better representation of extreme
outcomes at the expense of well-calibrated coverage (at least in set-ups where each input forecast
is already well-calibrated, which is likely to become the norm).