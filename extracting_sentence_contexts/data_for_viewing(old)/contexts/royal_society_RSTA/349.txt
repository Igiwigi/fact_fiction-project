return exactly the same components). Some nonlinearities have to be taken between different
layers. The connection between ICA and deep learning models is a very interesting topic for
future research.

7. Improvements in the estimation of linear decomposition

Finally, we will review methods for more efficient estimation of the basic linear mixing model
(2.2) when the components s; are independent as in the basic model assumptions.

(a) Independent component analysis using time-frequency decompositions

The basic ICA model assumes that the s; and x; are random variables, i.e. they have no time
structure. In the basic theory, it is in fact assumed that the observations are independent and
identically distribution (i.id.), as is typical in statistical theory. However, it is not at all necessary
that the components are i.id. for ICA to be meaningful. What the ii.d. assumption means in
practice is that any time structure of the data is ignored and what is analysed is simply the
marginal distribution of the data over time.

Nevertheless, it is clear that the time structure of the data could be useful for estimating
the components. In ยง6c, we already used it to model dependencies between the components,
but even in the case of completely independent components, time structure can provide more
information. In fact, it is sometimes possible to estimate the ICA model even for Gaussian data,
based on the time structure (autocorrelations) alone, as initially pointed out by Tong et al. [65]
and further developed by Belouchrani et al. [34], among others (see ch. 18 of Hyvarinen et al.
[3] or Yeredor [36] for reviews.) However, such methods based on autocorrelations alone have