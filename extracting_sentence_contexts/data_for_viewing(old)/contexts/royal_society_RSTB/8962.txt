a roll-out policy which is a default that is applied at so-far-
unexpanded leaves of the tree, and which is almost always
suboptimal but provably sufficient for this purpose. For the
case of prediction alone the roll-out policy is, in effect, what
it is sought to evaluate, and so the prediction tree could
have been built more aggressively, adding more than just a
single node per step at the root.

With respect to MF methods, we have seen that caching
quantities that depend on the rewards and transitions
inside a value function ¥, the successor matrix M or other
temporally extended representations can cause characteristic
errors in prediction when the model changes. The fact that
these quantities are both also policy-dependent (and also
that the policy itself is cached in methods like the actor—
critic) is another similar source of error. Thus, if the policy
changes at a state, this should typically induce value and
policy changes at other states. However, MF methods may
not update these all consistently. Notably, although in the
prediction case we suggested that the successor represen-
tation behaves like MB reinforcement learning with respect
to changes in the rewards r, though not the contingencies
T, in the full control case, the successor representation may
also fail to adjust properly to some changes in rewards
alone. This is because in the control case, changes in rewards