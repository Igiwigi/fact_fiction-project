treatments have to be both accurate and timely.

However, the biological and medical sciences do not neatly fit into this simple paradigm of
today’s science. These fields are rational but in the sense that, like pre-Galilean science, they
typically rationalize observations after the event. In the era of big data, with the rise of blind
data gathering, the biological and medical sciences increasingly recapitulate pre-Baconianism by
placing undue emphasis on blind data gathering without even post hoc explanations.

This rise of a modern version of pre-seventeenth century thinking, which places too
much emphasis on the power of haphazardly gathered data, is problematic for the following
reasons.

First, not all data are reliable. The fact that ‘most published research findings are false’, as
famously reported by John Ioannidis in PLOS Medicine [9], suggests one important dataset—the
conclusions of peer-reviewed studies—consists of predominantly bad data and cannot be relied
upon without evidence of good experimental design and rigorous statistical analysis.

Second, we need models and theoretical insights to help guide the collection, curation and
interpretation of data. The relatively meagre initial returns from the human genome demonstrate
that data do not translate readily into understanding, let alone treatments. Even using a rigorous
predictive statistical framework, characterizing average behaviour from big genomics data will
not deliver ‘personalized medicine’. This is now acknowledged by the rise of ‘precision medicine’,
which aims to link genes with pathologies in stratified populations.

Third, we have to take care when extrapolating beyond the range of existing data. The