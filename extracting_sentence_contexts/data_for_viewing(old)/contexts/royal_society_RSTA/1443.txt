prescribed tolerance, we must instead ask: What algorithms can reduce systematic errors to the
smallest possible values and at the same time be not so expensive as to be unaffordable? The
notion of what is affordable raises the question of how much society values accurate forecasts
of climate and whether climate simulation and prediction is of such importance societally that
it should be funded at a similar level of ambition to something like the Large Hadron Collider
[4]. We do not answer such questions here, but nevertheless suggest that the development of
algorithms which run in a reasonable time (say a day of wall-clock time for a decade of simulated
climate) on a dedicated exascale high-performance computer would appear a plausible objective
for the design of algorithms for the next-generation climate simulation and prediction models.

Since it is believed that the underpinning equations of motion on which the algorithms are
based are correct, an inability to solve these equations numerically without significant bias
must arise from the fact that existing numerical representations are not sufficiently accurate. In
practice, this means that the resolution of existing models is inadequate. As such, it should be
possible to reduce these errors by increasing the resolution of the model sufficiently, thereby
eliminating some of the semi-empirical subgrid parametrizations [5], which are required to
represent processes, which are not resolvable on the chosen grid. Currently, many climate models
have horizontal grid point spacings of around 100km, though some can run with finer grids
of around 20km. If it were possible to reduce grid spacing to around 1km grid, it would be
possible to eliminate parametrizations of deep convection and orographic gravity-wave drag in
the atmosphere and mesoscale eddy mixing in the oceans, and instead represent these processes
with the proper laws of physics. Unfortunately, even with the anticipated increase in computing
speed brought about by exascale computing, we will still be short of processing capability needed
to get toa 1 km grid by a couple of orders of magnitude. We need to find further cost savings.
