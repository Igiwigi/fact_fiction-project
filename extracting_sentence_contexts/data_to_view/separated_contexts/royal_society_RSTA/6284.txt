However, studies which seek to interpret the internal

IMy use of the words performance and competence is a deliberate re-imaging of Chomsky’s terminology. Chomsky argued that
the presence of language production errors in humans is not evidence against an internal representation of some abstract,
ideal grammatical structure. Similarly, the fact that we readily find examples of LLMs producing illogical or inconsistent
outputs should not alone be taken as evidence that they do not represent some more abstract or robust structure internally. If we focus on understanding this internal structure first, we might notice connections between processing in LLMs and
processing in humans which would be missed if we focused solely on LLMs’ errors in performance.


representations of neural networks have often revealed interpretable structures and processes, 3 |
reminscient of existing theories of cognition. Thus, the question of whether LLMs can inform our

theories of human language understanding is first and foremost an empirical question.