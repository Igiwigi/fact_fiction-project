of latent representation of situations and events. This is an This view is partially consistent with recent suggestions that © =

important feature, e.g. for processing the so-called Winograd language comprehension is sometimes just ‘good enough’ =

sentences such as ‘The trombone didn’t fit in the suitcase and does not always result in representations that are correct _

because it was too large/ small’ [48]. Humans correctly inter- in light of the syntax [56], without, importantly, sharing the = =

pret the word ‘it’ in this sentence as referring to the trombone implication of the phrase ‘good enough’ that the syntactically = 5!

if the last word is ‘large’ but to the suitcase if the last word is determined interpretation is necessarily the best one. In gen- >

‘small’. We believe that a semantic regularity—the fact that eral, it seems beneficial to rely on all possible sources of =

for an object (a) to fit into another object (b), (a) must be smaller information rather than giving overriding importance toa = &

than (b)—underlies this ability, and this is just the kind of regu- single consideration. In line with this view, rates of plausibility =

larity we believe a fully successful model based on the ideas based interpretations are higher when listening to speakers S

underlying the SG model should capture, while Winograd sen- with a foreign accent, suggesting that such interpretations are N

tences pose challenges for even the best current deep learning not a failure of the system, but rather result from a Bayes opti- s

models, for which training is purely language based [24]. mal process taking into account all available cues to best =