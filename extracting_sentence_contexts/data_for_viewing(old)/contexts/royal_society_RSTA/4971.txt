probable set of values for the [parameters] will make P a maximum’ [9]—though he does pull
back from this in the concluding section: ‘P is a relative probability only, suitable to compare
point with point, but incapable of being interpreted as a probability distribution over a region’
[9]. In this paper, he seems to be slowly clarifying in his mind the distinction between P seen as a
function of the data and P seen as a function of the parameters, a clarification which was complete
by 1922.

For example, in his 1922 paper he corrects his error and introduces a new term to clarify things:
‘I must indeed plead guilty in my original statement of the Method of the Maximum Likelihood
to having based my argument upon the principle of inverse probability ... Upon consideration,
therefore, I perceive that the word probability is wrongly used in such a connection: probability
is a ratio of frequencies, and about the frequencies of such values we can know nothing whatever.
We must return to the actual fact that one value of p, of the frequency of which we know nothing,
would yield the observed result three times as frequently as would another value of p ... Isuggest
that we may speak without confusion of the likelihood of one value of p being thrice the likelihood
of another’ [1, p. 326].

Again, the gradual refinement of his ideas is apparent in his other papers. At the end of a
paper published in 1921 [10], focusing on the distribution of the correlation coefficient, Fisher
added a ‘Note on the confusion between Bayes rule and my method of the evaluation of the
optimum’. He says, ‘My treatment of this problem differs radically from that of Bayes. Bayes
[11] attempted to find ... the actual probability that the population value lay in any given range.

... Such a problem is indeterminate without knowing the statistical mechanism under which
different values of p come into existence ... What we can find from a sample is the likelihood of