More recently, potential for infringement of the right to a fair trial by using an AI tool to assist
in sentencing has been recognized in US legal proceedings [10]. The Supreme Court of Wisconsin
considered the issue at length and issued detailed guidelines for future cases, the most important
of which is that fundamental rights will inevitably be infringed if decisions are made solely on the
basis of a machine learning technology which cannot give a satisfactory account of the process by
which its recommendations were produced ([10], paras 93-97). The fact that the tool’s predictions
were shown to be accurate enough in aggregate to justify its recommendations was an essential
prerequisite for using the tool at all ([10], paras 87-92), but did not overcome the obligation
to give individual consideration to the decision which thus required the tool to explain its
reasoning. The obvious regulatory response is to require each AI tool which has the potential to infringe
fundamental rights to be able to explain the reasoning leading to the tool’s decisions. This might
be workable for those AI tools where the risk is obvious, such as tools which assist in recruitment


decisions [11], but for many, the risk of their making infringing decisions will not be recognized
until an actual infringement occurs.