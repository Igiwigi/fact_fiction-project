Debater, [17]).

In the last few years, ‘transformer’ deep neural networks, with order 10!! trainable parameters
and trained on something close to the entire contents of the World Wide Web, have proved to be
remarkably successful in a wide range of natural language processing tasks, often showing highly
unexpected emergent behaviour.’ Perhaps most notably, so-called large language models, such a
GPT-3 from OpenAI [18], have produced some remarkable demonstrations.

To give a concrete example, artist Mario Klingemann® gave GPT-3 an author (Jerome K.
Jerome), a title (The importance of being on Twitter), and a first word (It) and obtained the
astonishing first paragraph:

It is a curious fact that the last remaining form of social life in which the people of London
are still interested is Twitter. I was struck with this curious fact when I went on one of my
periodical holidays to the sea-side, and found the whole place twittering like a starling-
cage. I called it an anomaly, and it is.

GPT-3 goes on to provide the rest of a ‘story’, though one that degenerates into increasing
narrative incoherence as the paragraphs progress.

As another illustration, the philosopher Henry Shevlin® prompted GPT-3 to engage in
a philosophical conversation, where GPT-3 had to adopt the persona of the well-known
philosopher of cognitive science, David Chalmers, generating some eerily compelling exchanges:

Shevlin: Dave, it’s great to have you here with us. First question—should we take