can encode compositional symbolic structure without inductive biases (induced through training
objective or architecture) towards such structure and (ii) that LLMs trained on text alone can
capture the essential parts of linguistic meaning. In each case, I support the claim with a
combination of in-principle arguments and empirical studies which inspect the internal structure
of the representations and processes within the neural network.

Overall, I argue that it is premature to presuppose categories of problems that models
like LLMs cannot or will not solve. Modern neural networks are in their infancy and our
understanding of how they work is nascent. However, studies which seek to interpret the internal

IMy use of the words performance and competence is a deliberate re-imaging of Chomsky’s terminology. Chomsky argued that
the presence of language production errors in humans is not evidence against an internal representation of some abstract,
ideal grammatical structure. Similarly, the fact that we readily find examples of LLMs producing illogical or inconsistent
outputs should not alone be taken as evidence that they do not represent some more abstract or robust structure internally.
If we focus on understanding this internal structure first, we might notice connections between processing in LLMs and
processing in humans which would be missed if we focused solely on LLMs’ errors in performance.


representations of neural networks have often revealed interpretable structures and processes, 3 |
reminscient of existing theories of cognition. Thus, the question of whether LLMs can inform our

theories of human language understanding is first and foremost an empirical question. And the 3
next decade (hopefully much less) of work on understanding how neural networks work under 1S

the hood is likely to move the needle significantly.