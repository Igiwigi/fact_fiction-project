{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets contexts for each keyword of interest\n",
    "(can speed this up by having it write the files as it runs, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "import spacy\n",
    "\n",
    "#might be a bit redundant to have both spacy and nltk, but can easily switch between them when testing (and remove one later)\n",
    "#spacy is better at splitting into sentences, but much slower performance\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.max_length = 3_000_000 #set a different max to accom. longer docs\n",
    "\n",
    "class TextContextExtractor:\n",
    "    def __init__(self, keywords, sentences_before, sentences_after, newest_year_included=1950, skip_after_specific_year=True, use_spacy=False, \n",
    "                  create_excel=True):\n",
    "        self.keywords = [kw.strip() for kw in keywords]\n",
    "        self.sentences_before = sentences_before #how many sentences before are counted\n",
    "        self.sentences_after = sentences_after #how many sentences after are counted\n",
    "        self.use_spacy = use_spacy #if not, uses nltk\n",
    "        self.newest_year_included = newest_year_included\n",
    "        self.create_excel = create_excel\n",
    "        self.skip_after_specific_year = skip_after_specific_year\n",
    "        \n",
    "        self.keyword_patterns = [\n",
    "            (kw, re.compile(r'\\b' + re.escape(kw) + r'\\b', flags=re.IGNORECASE))\n",
    "            for kw in self.keywords\n",
    "        ]\n",
    "\n",
    "    def extract_text_from_txt(self, txt_path):\n",
    "        \"\"\"Extract text from a given TXT file.\"\"\"\n",
    "        with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def _tokenize_sentences(self, text):\n",
    "        \"\"\"Tokenize text into sentences using either spaCy or NLTK.\"\"\"\n",
    "        if self.use_spacy:\n",
    "            doc = nlp(text)\n",
    "            sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "        else: #nltk\n",
    "            sentences = nltk.tokenize.sent_tokenize(text, language='english')\n",
    "            sentences = [sent.strip() for sent in sentences if sent.strip()]\n",
    "        return sentences\n",
    "\n",
    "    def _get_context_window(self, sentences, match_idx):\n",
    "        \"\"\"Extract a window of sentences around a matched keyword.\"\"\"\n",
    "        start_idx = max(0, match_idx - self.sentences_before)\n",
    "        end_idx = min(len(sentences), match_idx + 1 + self.sentences_after)\n",
    "        context_sentences = sentences[start_idx:end_idx]\n",
    "        return ' '.join(context_sentences)\n",
    "\n",
    "    def find_contexts(self, text):\n",
    "        \"\"\"\n",
    "        Extracts contextual snippets around specified keywords within a text. \n",
    "\n",
    "        How it works:\n",
    "        1. Splits the input text into sentences using either spaCy or NLTK tokenization.\n",
    "        2. Iterates over each keyword defined in keywords.\n",
    "        3. For each keyword, searches all sentences for matches (case-insensitive, \n",
    "        and matching whole words only to avoid partial matches).\n",
    "        4. When a keyword is found, selects a range of sentences around it:\n",
    "        - `sentences_before` sentences before the keyword sentence\n",
    "        - `sentences_after` sentences after the keyword sentence\n",
    "        5. Joins these sentences together into a single context snippet and stores \n",
    "        it along with the keyword.\n",
    "        \n",
    "        Returns:\n",
    "            A list of tuples: [(keyword1, context1), (keyword2, context2), ...]\n",
    "            Each tuple contains the keyword and the extracted context surrounding it.\n",
    "        \"\"\"\n",
    "        contexts = []\n",
    "        sentences = self._tokenize_sentences(text)\n",
    "        \n",
    "        for keyword, pattern in self.keyword_patterns:\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                if pattern.search(sentence):\n",
    "                    context = self._get_context_window(sentences, i)\n",
    "                    contexts.append((keyword, context))\n",
    "        return contexts\n",
    "\n",
    "    def save_contexts_to_csv(self, contexts, output_path):\n",
    "        \"\"\"Save extracted contexts to a CSV file.\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Filename\", \"Keyword\", \"Context\", \"Author\", \"Title\", \"Date\"])\n",
    "            writer.writerows(contexts)\n",
    "\n",
    "    def _format_as_text(self, value):\n",
    "        \"\"\"Ensure that the text is formatted as plain text in Excel.\"\"\"\n",
    "        if isinstance(value, str) and value.startswith('='):\n",
    "            return f\"'{value}\"\n",
    "        return value\n",
    "\n",
    "    def convert_csv_to_excel(self, input_csv, output_excel):\n",
    "        \"\"\"\n",
    "        Convert a CSV file to an Excel file, ensuring that context text is formatted as plain text in Excel.\n",
    "       \n",
    "        Parameters:\n",
    "        - input_csv (str): Path to the input CSV file.\n",
    "        - output_excel (str): Path to the output Excel file.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(input_csv)\n",
    "        df['Context'] = df['Context'].apply(self._format_as_text)\n",
    "        \n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(os.path.dirname(output_excel), exist_ok=True)\n",
    "       \n",
    "        df.to_excel(output_excel, index=False, engine='xlsxwriter')\n",
    "\n",
    "    def load_metadata_csv(self, metadata_path):\n",
    "        \"\"\"Load metadata from a CSV file into a dictionary.\"\"\"\n",
    "        metadata = {}\n",
    "        with open(metadata_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                if 'filename' in row:\n",
    "                    file_location = row['filename']\n",
    "                else:\n",
    "                    file_location = row.get('pdf_link', '').split('/')[-1]\n",
    "                \n",
    "                base_name = os.path.splitext(file_location)[0]\n",
    "                metadata[file_location] = row\n",
    "                metadata[base_name + '.txt'] = row\n",
    "                metadata[base_name + '.pdf'] = row\n",
    "        return metadata\n",
    "\n",
    "    def _get_file_metadata(self, metadata, filename):\n",
    "        \"\"\"Get metadata for a file, trying multiple filename formats.\"\"\"\n",
    "        #messy way of getting the pdf_filename between diff types of metadata\n",
    "        #could be improved but works for now\n",
    "        \n",
    "        if filename in metadata:\n",
    "            return metadata[filename]\n",
    "        \n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        pdf_filename = base_name + \".pdf\"\n",
    "        txt_filename = base_name + \".txt\"\n",
    "        \n",
    "        if pdf_filename in metadata:\n",
    "            return metadata[pdf_filename]\n",
    "        elif txt_filename in metadata:\n",
    "            return metadata[txt_filename]\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _should_skip_by_year(self, date):\n",
    "        \"\"\"Check if a file should be skipped based on its year.\"\"\"\n",
    "        year_match = re.search(r'\\d{4}', str(date))\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            if year > self.newest_year_included:\n",
    "                #print(f\"{filename} article, year {year}, is too new to be of interest (newer than year:{newest_year_included})\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _create_excel_path(self, output_csv):\n",
    "        \"\"\"Generate the Excel output path in the excel_ver subfolder.\"\"\"\n",
    "        csv_dir = os.path.dirname(output_csv)\n",
    "        csv_basename = os.path.splitext(os.path.basename(output_csv))[0]\n",
    "        excel_dir = os.path.join(csv_dir, 'excel_ver')\n",
    "        output_excel = os.path.join(excel_dir, csv_basename + '.xlsx') #one folder down from the csv\n",
    "        #output_excel = os.path.join(csv_basename + '.xlsx') #if in same folder\n",
    "        return output_excel\n",
    "\n",
    "    def process_texts(self, folder_path, output_csv, metadata_csv):\n",
    "        \"\"\"\n",
    "        Extract keyword contexts from all text files in a folder, enrich them with \n",
    "        metadata (extracted from data before in R), and save to a CSV. \n",
    "        Optionally converts the CSV to Excel.\n",
    "\n",
    "        Steps:\n",
    "        1. Loads metadata from a CSV file into a dictionary.\n",
    "        2. Iterates through each TXT file in `folder_path`.\n",
    "        3. Extracts text, finds contexts around keywords, and combines with metadata \n",
    "        (author, title, date).\n",
    "        4. Saves all contexts to `output_csv`.\n",
    "        5. Optionally creates an Excel version in a subfolder 'excel_ver'.\n",
    "\n",
    "        Args:\n",
    "            folder_path (str): Folder containing text files to process.\n",
    "            output_csv (str): Path to save the combined contexts CSV.\n",
    "            metadata_csv (str): Path to CSV containing metadata for the files.\n",
    "        \"\"\"\n",
    "        metadata = self.load_metadata_csv(metadata_csv)\n",
    "        all_contexts = []\n",
    "\n",
    "        txt_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "        print(f\"Beginning processing {len(txt_files)} text files in folder: {folder_path}\")\n",
    "\n",
    "        for filename in tqdm(txt_files, desc=\"Processing files\", unit=\"file\"):\n",
    "            txt_path = os.path.join(folder_path, filename)\n",
    "            text = self.extract_text_from_txt(txt_path)\n",
    "            \n",
    "            file_metadata = self._get_file_metadata(metadata, filename)\n",
    "            \n",
    "            if not file_metadata:\n",
    "                print(f\"No metadata found for {filename}\")\n",
    "                continue\n",
    "\n",
    "            author = file_metadata.get('author', '')\n",
    "            title = file_metadata.get('title', '')\n",
    "            date = file_metadata.get('date', '')\n",
    "\n",
    "            #skip after specific year (too new stuff shouldn't be included); the filename and year are actually somewhat different, might have been scraped metadata Date\n",
    "            if self.skip_after_specific_year:\n",
    "                if self._should_skip_by_year(date):\n",
    "                    continue\n",
    "            \n",
    "            contexts = self.find_contexts(text)\n",
    "\n",
    "            for keyword, context in contexts:\n",
    "                all_contexts.append((filename, keyword, context, author, title, date))\n",
    "\n",
    "        self.save_contexts_to_csv(all_contexts, output_csv) #might not need both csv and excel\n",
    "        \n",
    "        # Optionally create Excel file in excel_ver subfolder (easier to work with/possibly read)\n",
    "        # should be a separate function\n",
    "        if self.create_excel:\n",
    "            output_excel = self._create_excel_path(output_csv)\n",
    "            self.convert_csv_to_excel(output_csv, output_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"fact\", \"fiction\", \"facts\", \"facſ\", \"fictions\", \"ficſions\", \"factual\", \"fictional\", \"fictionally\", \"factually\", \"fictionality\", \"factuality\", \"fictionalized\", \"factualized\", \"fictive\", \"factive\", \"fictitious\", \"factious\"]\n",
    "#could add stemming instead of hardcoding them (so fact/facts automatically)\n",
    "\n",
    "extractor = TextContextExtractor(\n",
    "    keywords=keywords,\n",
    "    sentences_before=2,\n",
    "    sentences_after=2,\n",
    "    newest_year_included=1950,\n",
    "    skip_after_specific_year=True,\n",
    "    use_spacy=True, #nltk if False\n",
    "    create_excel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning processing 4390 text files in folder: D:/Fact_fiction_corpus/texts/royal society/txt_rsta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  69%|██████▉   | 3049/4390 [00:00<00:00, 6051.38file/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x0000025399AD8F10>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Igiba\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 788, in _clean_thread_parent_frames\n",
      "    if phase != \"start\":\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "#RSTA (#royal society)\n",
    "extractor.process_texts(\n",
    "    folder_path=\"D:/Fact_fiction_corpus/texts/royal society/txt_rsta\",\n",
    "    output_csv=\"../data_to_view/contexts_all_together/contexts_RSTA.csv\",\n",
    "    metadata_csv=\"D:/Fact_fiction_corpus/texts/royal society/royalsociety_metadata_rsta.csv\"\n",
    ")\n",
    "\n",
    "#RSTB (#royal society)\n",
    "extractor.process_texts(\n",
    "    folder_path=\"D:/Fact_fiction_corpus/texts/royal society/txt_rstb\",\n",
    "    output_csv=\"../data_to_view/contexts_all_together/contexts_RSTB.csv\",\n",
    "    metadata_csv=\"D:/Fact_fiction_corpus/texts/royal society/royalsociety_metadata_rstb.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning processing 8520 text files in folder: D:/Fact_fiction_corpus/texts/royal society/txt_rstl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 8520/8520 [1:45:38<00:00,  1.34file/s]  \n"
     ]
    }
   ],
   "source": [
    "#RSTL (#royal society)\n",
    "extractor.process_texts(\n",
    "    folder_path=\"D:/Fact_fiction_corpus/texts/royal society/txt_rstl\",\n",
    "    output_csv=\"../data_to_view/contexts_all_together/contexts_RSTL.csv\",\n",
    "    metadata_csv=\"D:/Fact_fiction_corpus/texts/royal society/royalsociety_metadata_rstl.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning processing 1 text files in folder: D:/Fact_fiction_corpus/texts/General Magazine of Arts and Sciences/txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/1 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/1 [00:00<?, ?file/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1025006 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#General Magazine of Arts and Sciences (albeit empty)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:/Fact_fiction_corpus/texts/General Magazine of Arts and Sciences/txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data_to_view/contexts_all_together/contexts_general_magazine.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:/Fact_fiction_corpus/texts/General Magazine of Arts and Sciences/general_magazine_metadata.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 219\u001b[0m, in \u001b[0;36mTextContextExtractor.process_texts\u001b[1;34m(self, folder_path, output_csv, metadata_csv)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_skip_by_year(date):\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m contexts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword, context \u001b[38;5;129;01min\u001b[39;00m contexts:\n\u001b[0;32m    222\u001b[0m     all_contexts\u001b[38;5;241m.\u001b[39mappend((filename, keyword, context, author, title, date))\n",
      "Cell \u001b[1;32mIn[1], line 80\u001b[0m, in \u001b[0;36mTextContextExtractor.find_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03mExtracts contextual snippets around specified keywords within a text. \u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    Each tuple contains the keyword and the extracted context surrounding it.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m contexts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 80\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword, pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeyword_patterns:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentences):\n",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m, in \u001b[0;36mTextContextExtractor._tokenize_sentences\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Tokenize text into sentences using either spaCy or NLTK.\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_spacy:\n\u001b[1;32m---> 46\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [sent\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents \u001b[38;5;28;01mif\u001b[39;00m sent\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#nltk\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\spacy\\language.py:1041\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1022\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1026\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1041\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\spacy\\language.py:1132\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\spacy\\language.py:1121\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \n\u001b[0;32m   1117\u001b[0m \u001b[38;5;124;03mtext (str): The text to process.\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;124;03mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1122\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m   1123\u001b[0m     )\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n",
      "\u001b[1;31mValueError\u001b[0m: [E088] Text of length 1025006 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "#General Magazine of Arts and Sciences (albeit empty)\n",
    "extractor.process_texts(\n",
    "    folder_path=\"D:/Fact_fiction_corpus/texts/General Magazine of Arts and Sciences/txt\",\n",
    "    output_csv=\"../data_to_view/contexts_all_together/contexts_general_magazine.csv\",\n",
    "    metadata_csv=\"D:/Fact_fiction_corpus/texts/General Magazine of Arts and Sciences/general_magazine_metadata.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Spectator\n",
    "extractor.process_texts(\n",
    "    folder_path=\"D:/Fact_fiction_corpus/texts/spectator/txt\",\n",
    "    output_csv=\"../data_to_view/contexts_all_together/contexts_spectator.csv\",\n",
    "    metadata_csv=\"D:/Fact_fiction_corpus/texts/spectator/spectator_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: rsta20230324.txt\n",
      "Keyword: fact\n",
      "Author: J. M. Londono Monsalve and E. Kovalska and M. F. Craciun and M. R. Marsico\n",
      "Title: Graphene nanoplatelets on recycled rubber an experimental study of material properties and mechanical improvements\n",
      "Context: Adherence among layers has been assessed qualitatively by visually\n",
      "inspecting coated and uncoated specimens cut in half after curing, with curing temperature and\n",
      "time have been adjusted to ensure layer bonding. Adherence has also been confirmed through\n",
      "the deformed shape of the specimen during the shear test as shown in figure 10. In this test,\n",
      "since the composite is layered parallel to the plates’ direction, one would expect the layers to\n",
      "slide with respect to each other in the absence of bonding, thus, the fact that the specimens\n",
      "deform as a solid block implies layer bonding. Table 1. Specimens for morphology and chemical characterization.\n",
      "\n",
      "Filename: rsta20230324.txt\n",
      "Keyword: fact\n",
      "Author: J. M. Londono Monsalve and E. Kovalska and M. F. Craciun and M. R. Marsico\n",
      "Title: Graphene nanoplatelets on recycled rubber an experimental study of material properties and mechanical improvements\n",
      "Context: We believe this\n",
      "is due to the high unevenness of the surface of the RR substrate that can be appreciated in the\n",
      "three-dimensional image presented in figure 5. Graphene was inevitably deposited on the mask during the deposition. In order to estimate\n",
      "the thickness of the graphene nanoplatelets’ cumulative cycles, an analysis of a scratch made\n",
      "on the coated mask was carried out, taking advantage of the fact that the surface of the brass\n",
      "mask is more uniform compared with the surface of the rubber. Figure 6a,b shows the profiles\n",
      "at 200x magnification traced on the scratched area and on the graphene-coated area used to\n",
      "estimate the thickness of the graphene coating. The gap between the two topographic profiles\n",
      "is around 11 «1m, which was estimated to be the thickness of the 150 cumulative cycles of\n",
      "graphene nanoplatelets.\n",
      "\n",
      "Filename: rsta_2022_0143.txt\n",
      "Keyword: fact\n",
      "Author: Chris C. Holmes and Stephen G. Walker\n",
      "Title: Statistical inference with exchangeability and martingales\n",
      "Context: Lindley & Smith [5]) obtained\n",
      "in the limit of large data, without assuming that the true data generating process is a linear\n",
      "model. The conventional Bayesian approach conflates these two issues, that the estimate refers\n",
      "to parameters in the predictive model assumed to be true (see §3). (a) Martingales\n",
      "\n",
      "In fact, martingales are important for this view of the Bayesian methodology. Consider the\n",
      "\n",
      "posterior mean conditional on x}.741, Le. J Of Cn41 | 0) 10 | X10) dO\n",
      "POn+1 | X1:n)\n",
      "\n",
      "Ova = | O10 |xinsa) a0\n",
      "where x,,41 has been taken from the predictive p(x,41 | X1:n).\n",
      "\n",
      "Filename: rsta_2015_0174.txt\n",
      "Keyword: fact\n",
      "Author: Reuven Segev\n",
      "Title: Continuum mechanics, stresses, currents and electrodynamics\n",
      "Context: 1. Introduction\n",
      "Author for correspondence: In this paper, we make an attempt to provide\n",
      "Reuven Segev a mathematical setting that is general enough to\n",
      "\n",
      "encompass both stress theory in continuum mechanics\n",
      "and classical field theories of physics. In fact, the\n",
      "proposed setting generalizes both. In the continuum\n",
      "mechanics context, we propose a weak formulation of\n",
      "the fundamentals of continuum mechanics that does not\n",
      "use the traditional notion of a material point. In the\n",
      "context of field theories, we present a weak formulation\n",
      "of p-form electrodynamics.\n",
      "\n",
      "Filename: rsta_2015_0174.txt\n",
      "Keyword: fact\n",
      "Author: Reuven Segev\n",
      "Title: Continuum mechanics, stresses, currents and electrodynamics\n",
      "Context: It has been recognized that, in general, bodies\n",
      "need not have natural reference configurations in space, and, as such, they should be represented\n",
      "\n",
      "mathematically as differentiable manifolds. The notion of a body point makes sense because of the : =\n",
      "principle of material impenetrability which implies that the configurations of a body in space are : Sy\n",
      "assumed to be embeddings. In fact, as early as [2], the configurations of a body in space were a\n",
      "described as charts on the body manifold. We will refer to this point of view of continuum aed\n",
      "mechanics as the Lagrangian approach. 22\n",
      "\n",
      "A basic notion of force theory in continuum mechanics is the concept of the stress tensor.\n",
      "\n",
      "Filename: rsta_2015_0174.txt\n",
      "Keyword: fact\n",
      "Author: Reuven Segev\n",
      "Title: Continuum mechanics, stresses, currents and electrodynamics\n",
      "Context: The evaluation of the stress object on the\n",
      "derivative of the generalized velocity, as represented by the jet of the corresponding vector field,\n",
      "is equal to the power of the force functional—a generalization of the principle of virtual work\n",
      "in continuum mechanics. Just as one would expect, without any constitutive information, a force\n",
      "does not determine a unique representing stress. On the other hand, it is emphasized that the\n",
      "significance of the representation of forces by measures stems from the fact that measures are the\n",
      "most irregular objects that may be naturally restricted to sub-bodies of the body. As such, a stress\n",
      "induces a force system in the body. In spite of the advantages of the Lagrangian point of view, the notion of a material point,\n",
      "which is conserved in all configurations of a body, is not general enough (e.g.\n",
      "\n",
      "Filename: rsta_2015_0174.txt\n",
      "Keyword: fact\n",
      "Author: Reuven Segev\n",
      "Title: Continuum mechanics, stresses, currents and electrodynamics\n",
      "Context: may be\n",
      "\n",
      "\n",
      "extended to an element S € C°(J!W)*. By the Riesz representation theorem, such an element S is a\n",
      "measure which is valued in the dual of the jet bundle. If a different atlas had been chosen, the fact\n",
      "that equivalent topologies would be induced on the spaces of sections implies that F o (j! )~1 is\n",
      "still continuous. The construction is therefore independent of the choice of an atlas.\n",
      "\n",
      "Filename: rsta_2015_0174.txt\n",
      "Keyword: fact\n",
      "Author: Reuven Segev\n",
      "Title: Continuum mechanics, stresses, currents and electrodynamics\n",
      "Context: It is observed that, if the density of the extensive property is given by\n",
      "an n-form 6 which does not vanish where w does not vanish, there is a unique vector field w\n",
      "satisfying the condition # = w. 6. Evidently, the vector field w may be interpreted as a velocity\n",
      "field and its integral lines may be interpreted as body points. In fact, even if no specific volume\n",
      "element 6 is given, the flux form w determines a family of one-dimensional submanifolds which\n",
      "one may interpret as body points [16]. Rather than restricting ourselves to sections of A\" TY, as suggested by the discussion\n",
      "above, we consider for now a general vector bundle z : W — .Y. In accordance with the Eulerian\n",
      "point of view, we model a force functional F as a continuous, linear functional on the space ch)\n",
      "of differentiable sections of W having compact supports.\n",
      "\n",
      "Filename: rsta_2015_0190.txt\n",
      "Keyword: fact\n",
      "Author: Guo-Rong Wu and Daniele Marinazzo\n",
      "Title: Sensitivity of the resting-state haemodynamic response function estimation to autonomic nervous system fluctuations\n",
      "Context: Apart from the variation of amplitude in BOLD signal,\n",
      "additional temporal characteristics of the haemodynamic response, not available from tSNR and\n",
      "FC maps such as time to peak, could be revealed by statistical analysis of spontaneous point\n",
      "process HRF. Unlike thermal noise, physiological fluctuations can introduce fluctuations in the fMRI signal\n",
      "that are uncoupled from neural activity, and are among the most important confounds in BOLD\n",
      "signal change [17]. In fact, cardiac mechanisms include changes in cerebral blood flow/volume\n",
      "and arterial pulsatility [18]. Respiration effects include changes in Bo and arterial CO2 partial\n",
      "pressure [19]. Although cardiac and respiratory cycles have relatively high frequencies in contrast\n",
      "to the typical low-frequency (less than 0.1Hz) BOLD fluctuations, aliasing of physiological\n",
      "components to lower frequency range will inevitably occur owing to lower sampling rate in\n",
      "BOLD fMRI than cardiac and respiratory cycles [20].\n",
      "\n",
      "Filename: rsta_2015_0190.txt\n",
      "Keyword: fact\n",
      "Author: Guo-Rong Wu and Daniele Marinazzo\n",
      "Title: Sensitivity of the resting-state haemodynamic response function estimation to autonomic nervous system fluctuations\n",
      "Context: In\n",
      "addition, LF power, which is generally thought to be modulated by both sympathetic and\n",
      "parasympathetic activity, is robustly correlated with the response duration in the midbrain. These phenomena are not affected by different processing pipelines, but cannot be evidenced\n",
      "with longer TRs. This may be explained by the fact that a more precise estimation of\n",
      "haemodynamic response duration requires a higher sample rate. To further confirm the effect\n",
      "from different magnitude field strength, a short TR acquisition with a 7T MRI scanner would be\n",
      "a great resource. With longer TRs, the associations between HRF parameters and HRV are more sensitive to the\n",
      "processing steps.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing the output txt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data_to_view/contexts_all_together/contexts_RSTA.csv\")\n",
    "\n",
    "for index, row in df.head(10).iterrows():\n",
    "    print(f\"Filename: {row['Filename']}\")\n",
    "    print(f\"Keyword: {row['Keyword']}\")\n",
    "    print(f\"Author: {row['Author']}\")\n",
    "    print(f\"Title: {row['Title']}\")\n",
    "    print(f\"Context: {row['Context']}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system(f\"shutdown /s /t 60\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
