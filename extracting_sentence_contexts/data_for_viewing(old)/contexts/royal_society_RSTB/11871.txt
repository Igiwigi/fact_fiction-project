case. In one study [30], for instance, it was shown that people lish a sense of commitment between itself and Jack; however,
judge a robot to be more capable when that robot is able to Jack may have resisted the temptation to disengage and seek
perform conversational speech. This implies that there is human help, thus mitigating the damage to the reputation of
more room for disappointment when a robot appears the shop’s quality of service.
human-like and thereby raises the expectation of a smooth It should be acknowledged that error-prone robots may
and effective interaction. Interestingly, Cha et al. [30] also also be endearing and evoke empathy in virtue of their
showed that people felt robots with conversational speech errors, leading to increased patience on the part of human
were less capable after a failure than robots who did not interactants. In fact, there is evidence pointing in this direc-
have such conversational functionality. Thirdly, there is a tion from one study [10], showing that participants actually
further reason why people may incorrectly perceive robots favoured robots who made mistakes, as this made them
to be underperforming—i.e. people are typically unfamiliar more human-like. However, the point stands that many cus-
with the workings of robots and may sometimes misjudge tomers in a retail scenario may be more impatient and more
that a robot has broken down when, in fact, it is taking demanding when they are shopping for something than
longer to perform an action than expected simply because when they are interacting with a robot in an experimental
it is recalibrating or exploring the learning space [31]. scenario.
Fourth, there may be important negative consequences Consider a different type of case: human disengagement
of human disengagement in response to (perceived) robot in response to robot overperformance. While we are not
underperformance. aware of any research directly investigating this possibility

The negative consequences of human disengagement in the context of human-robot interaction, there has been rel-
may range from opportunity costs (e.g. the human decides evant research in the context of human reliance on automated
to go to a different shop or service provider where she/he systems [36]. For example, it has been shown that pilots’
can interact with a reason person), to subpar productivity excessive trust in or overreliance on autopilot systems can
in industry settings involving human-robot interaction, to lead to dangerous monitoring failures [37]. Similarly, many
health and safety risks if the human interactant ceases road vehicles already contain computational systems that