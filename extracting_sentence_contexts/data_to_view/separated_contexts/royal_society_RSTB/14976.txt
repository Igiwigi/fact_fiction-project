Results are
impressive, but they also highlight inherent limitations, particularly the flexibility in transferring knowledge from one set of
data to another, which also bring about states of ‘artificial hallucinations’ (i.e. generated responses that are either factually
incorrect, non-sensical or not grounded in reality (e.g. [66-68]). Current LLM systems do not implement forms of active learning and autonomous engagement with the world’s affordances. Even if the system could be able to recover facts about the world’s structure, the recovered facts are explicitly based on the
4E human experience, hence the depiction of LLMs as ‘stochastic parrots’, due to their pattern-matching approach to the
language detailed above [69].* Others consider LLMs only an outward display of intelligence: similarly to Searle’s ‘Chinese
room’, they give the impression of comprehending only because they can match incoming input symbols with specific output
symbols [74]. Obviously, it is not possible to say that disembodied entities such as LLMs could never achieve understanding or
agency in some sense. The production of systems able to describe the reasons for their decisions (i.e. explainability) in natural
language, as a pragmatic premise to overt discourse, is a major avenue for Als’ development [14—16].